{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5879e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/connectome/jubin/ABCD-3DCNN/STEP_4_Multimodal-Learning/MultiChannel-Learning/contrastive_learning/codes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('/scratch/connectome/jubin/ABCD-3DCNN/STEP_4_Multimodal-Learning/MultiChannel-Learning/contrastive_learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14dc13f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--debug'], dest='debug', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse \n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Options for model setting\n",
    "parser.add_argument(\"--model\", type=str, required=True, help='Select model. e.g. densenet3D121, sfcn.',\n",
    "                    choices=['simple3D', 'sfcn', 'vgg3D11', 'vgg3D13', 'vgg3D16', 'vgg3D19',\n",
    "                             'resnet3D50', 'resnet3D101', 'resnet3D152',\n",
    "                             'densenet3D121', 'densenet3D169', 'densenet201', 'densenet264'])\n",
    "parser.add_argument(\"--in_channels\", default=1, type=int, help='')\n",
    "\n",
    "# Options for dataset and data type, split ratio, CV, resize, augmentation\n",
    "parser.add_argument(\"--dataset\", type=str, choices=['UKB','ABCD'], required=True, help='Selelct dataset')\n",
    "parser.add_argument(\"--data_type\", nargs='+', type=str, help='Select data type(sMRI, dMRI)',\n",
    "                    choices=['fmriprep', 'freesurfer', 'freesurfer_256', 'FA_unwarpped_nii', 'FA_warpped_nii',\n",
    "                             'MD_unwarpped_nii', 'MD_warpped_nii', 'RD_unwarpped_nii', 'RD_warpped_nii'])\n",
    "parser.add_argument(\"--val_size\", default=0.1, type=float, help='')\n",
    "parser.add_argument(\"--test_size\", default=0.1, type=float, help='')\n",
    "parser.add_argument(\"--cv\", default=None, type=int, choices=[1,2,3,4,5], help=\"option for 5-fold CV. 1~5.\")\n",
    "parser.add_argument(\"--resize\", nargs=\"*\", default=(96, 96, 96), type=int, help='')\n",
    "parser.add_argument(\"--augmentation\", nargs=\"*\", default=[], type=str, choices=['shift','flip'],\n",
    "                    help=\"Data augmentation - [shift, flip] are available\")\n",
    "\n",
    "# Hyperparameters for model training\n",
    "parser.add_argument(\"--lr\", default=0.01, type=float, help='')\n",
    "parser.add_argument(\"--lr_adjust\", default=0.01, type=float, help='')\n",
    "parser.add_argument(\"--epoch\", type=int, required=True, help='')\n",
    "parser.add_argument(\"--epoch_FC\", type=int, default=0, help='Option for training only FC layer')\n",
    "parser.add_argument(\"--optim\", default='Adam', type=str, choices=['Adam','SGD','RAdam','AdamW'], help='')\n",
    "parser.add_argument(\"--weight_decay\", default=0.001, type=float, help='')\n",
    "parser.add_argument(\"--scheduler\", default='', type=str, help='') \n",
    "parser.add_argument(\"--early_stopping\", default=None, type=int, help='')\n",
    "parser.add_argument(\"--train_batch_size\", default=16, type=int, help='')\n",
    "parser.add_argument(\"--val_batch_size\", default=16, type=int, help='')\n",
    "parser.add_argument(\"--test_batch_size\", default=1, type=int, help='')\n",
    "\n",
    "# Options for experiment setting\n",
    "parser.add_argument(\"--exp_name\", type=str, required=True, help='')\n",
    "parser.add_argument(\"--gpus\", nargs='+', type=int, help='')\n",
    "parser.add_argument(\"--sbatch\", type=str, choices=['True', 'False'])\n",
    "parser.add_argument(\"--cat_target\", nargs='+', default=[], type=str, help='')\n",
    "parser.add_argument(\"--metric\", default='cos', type=str, choices=['cos', 'L2'],\n",
    "                    help='Option for similarity loss metric')\n",
    "parser.add_argument(\"--num_target\", nargs='+', default=[], type=str, help='')\n",
    "parser.add_argument(\"--confusion_matrix\",  nargs='*', default=[], type=str, help='')\n",
    "parser.add_argument(\"--filter\", nargs=\"*\", default=[], type=str,\n",
    "                    help='options for filter data by phenotype. usage: --filter abcd_site:10 sex:1')\n",
    "parser.add_argument(\"--load\", default='', type=str, help='Load model weight that mathces {your_exp_dir}/result/*{load}*')\n",
    "parser.add_argument(\"--scratch\", default='', type=str, help='Option for learning from scratch')\n",
    "parser.add_argument(\"--transfer\", default='', type=str, choices=['sex','age','simclr','MAE'],\n",
    "                    help='Choose pretrained model according to your option')\n",
    "parser.add_argument(\"--unfrozen_layer\", default='0', type=str, help='Select the number of layers that would be unfrozen')\n",
    "parser.add_argument(\"--init_unfrozen\", default='', type=str, help='Initializes unfrozen layers')\n",
    "parser.add_argument(\"--debug\", default='', type=str, help='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722d2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"sex\"  #Attention.Deficit.Hyperactivity.Disorder.x\n",
    "data_type=\"freesurfer_256 FA_warpped_nii\"\n",
    "model=\"sfcn\"\n",
    "resize=\"80 80 80\"\n",
    "epoch_FC=\"0\"\n",
    "epoch=\"20\"\n",
    "optim=\"AdamW\"\n",
    "scheduler=\"--scheduler on\" # step_80\"\n",
    "batch=\"32\" \n",
    "val_size=\"0.25\"\n",
    "test_size=\"0.25\"\n",
    "lr=\"1e-3\"\n",
    "lr_adjust=\"--lr_adjust 1\"\n",
    "cfm=\"--confusion_matrix sex\" #--confusion_matrix Attention.Deficit.Hyperactivity.Disorder.x\n",
    "unfrozen=\"all\" \n",
    "weight_decay=\"--weight_decay 0.01\"\n",
    "metric='L2'\n",
    "#early_stopping=\"--early_stopping 20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf96617",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm='''${transfer} --cat_target ${target} --dataset ABCD --data_type ${data_type} --val_size ${val_size} --test_size ${test_size} ${scratch} ${filter} ${aug}\\\n",
    "    --lr ${lr} ${lr_adjust} --optim ${optim} ${weight_decay} --resize ${resize} ${scheduler} --train_batch_size ${batch} --val_batch_size ${batch} ${cfm} ${cv}\\\n",
    "    --exp_name ${exp_name} --model ${model} --epoch ${epoch} --epoch_FC ${epoch_FC} --unfrozen_layer ${unfrozen} --sbatch True ${load} ${early_stopping}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9729047",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{transfer} --cat_target {target} --dataset ABCD --data_type {data_type} --val_size {val_size} --test_size {test_size} {scratch} {filter} {aug}    --lr {lr} {lr_adjust} --optim {optim} {weight_decay} --resize {resize} {scheduler} --train_batch_size {batch} --val_batch_size {batch} {cfm} {cv}    --exp_name {exp_name} --model {model} --epoch {epoch} --epoch_FC {epoch_FC} --unfrozen_layer {unfrozen} --sbatch True {load} {early_stopping}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm = comm.replace(\"$\",'')\n",
    "comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e839c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--cat_target {target} --dataset ABCD --data_type {data_type} --val_size {val_size} --test_size {test_size} --lr {lr} --optim {optim} --resize {resize} --train_batch_size {batch} --val_batch_size {batch} --exp_name {exp_name} --model {model} --epoch {epoch} --epoch_FC {epoch_FC} --unfrozen_layer {unfrozen} --sbatch True'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com = list(filter(lambda x: x != '', comm.split(\" \")))\n",
    "new = []\n",
    "prev=False\n",
    "for c in com.copy():\n",
    "    curr = c.startswith(\"-\")\n",
    "    if curr or prev:\n",
    "        new.append(c)\n",
    "    prev = curr\n",
    "' '.join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83df12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "com = f'--metric L2 --cat_target {target} --dataset ABCD --data_type {data_type} --val_size {val_size} --test_size {test_size} \\\n",
    "--lr {lr} --optim {optim} --resize {resize} --train_batch_size {batch} --val_batch_size {batch} \\\n",
    "--exp_name act_func_test1 --model {model} --epoch {epoch} --epoch_FC {epoch_FC} --unfrozen_layer {unfrozen} \\\n",
    "--gpus 0 --debug 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7855cb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--metric L2 --cat_target sex --dataset ABCD --data_type freesurfer_256 FA_warpped_nii --val_size 0.25 --test_size 0.25 --lr 1e-3 --optim AdamW --resize 80 80 80 --train_batch_size 32 --val_batch_size 32 --exp_name act_func_test1 --model sfcn --epoch 20 --epoch_FC 0 --unfrozen_layer all --gpus 0 --debug 1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a2353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Categorical target labels are ['sex'] and Numerical target labels are [] *** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(com.split())\n",
    "if args.cat_target == args.num_target:\n",
    "    raise ValueError('--num-target or --cat-target should be specified')\n",
    "\n",
    "print(f\"*** Categorical target labels are {args.cat_target} and Numerical target labels are {args.num_target} *** \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922d7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SMU: smooth activation function for deep networks using smoothing maximum technique(https://arxiv.org/abs/2111.04682)\n",
    "## https://github.com/iFe1er/SMU_pytorch/blob/main/smu.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SMU(nn.Module):\n",
    "    '''\n",
    "    Implementation of SMU activation.\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    Parameters:\n",
    "        - alpha: hyper parameter\n",
    "    References:\n",
    "        - See related paper:\n",
    "        https://arxiv.org/abs/2111.04682\n",
    "    Examples:\n",
    "        >>> smu = SMU()\n",
    "        >>> x = torch.Tensor([0.6,-0.3])\n",
    "        >>> x = smu(x)\n",
    "    '''\n",
    "    def __init__(self, alpha = 0.01, mu = 2.5):\n",
    "        '''\n",
    "        Initialization.\n",
    "        INPUT:\n",
    "            - alpha: hyper parameter\n",
    "            aplha is initialized with zero value by default\n",
    "        '''\n",
    "        super(SMU,self).__init__()\n",
    "        self.alpha = alpha\n",
    "        # initialize mu\n",
    "        self.mu = torch.nn.Parameter(torch.tensor(mu)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ((1+self.alpha)*x + (1-self.alpha)*x*torch.erf(self.mu*(1-self.alpha)*x))/2\n",
    "        \n",
    "        \n",
    "class SMU1(nn.Module):\n",
    "    '''\n",
    "    Implementation of SMU-1 activation.\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    Parameters:\n",
    "        - alpha: hyper parameter\n",
    "    References:\n",
    "        - See related paper:\n",
    "        https://arxiv.org/abs/2111.04682\n",
    "    Examples:\n",
    "        >>> smu1 = SMU1()\n",
    "        >>> x = torch.Tensor([0.6,-0.3])\n",
    "        >>> x = smu1(x)\n",
    "    '''\n",
    "    def __init__(self, alpha = 0.01, mu = 4.332461424154261e-9):\n",
    "        '''\n",
    "        Initialization.\n",
    "        INPUT:\n",
    "            - alpha: hyper parameter\n",
    "            aplha is initialized with zero value by default\n",
    "        '''\n",
    "        super(SMU1,self).__init__()\n",
    "        self.alpha = alpha\n",
    "        # initialize mu\n",
    "        self.mu = torch.nn.Parameter(torch.tensor(mu)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ((1+self.alpha)*x+torch.sqrt(torch.square(x-self.alpha*x)+torch.square(self.mu)))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d25fc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "## =================================== ##\n",
    "## ======= DenseNet ======= ##\n",
    "## =================================== ##\n",
    "\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "# utils\n",
    "import collections\n",
    "\n",
    "## ========= DenseNet Model ========= #\n",
    "#(ref) explanation - https://wingnim.tistory.com/39\n",
    "#(ref) densenet3d - https://github.com/pytorch/vision/blob/main/torchvision/models/densenet.py\n",
    "#(ref) pytorch - https://pytorch.org/vision/0.8/_modules/torchvision/models/densenet.html\n",
    "\n",
    "class _DenseLayerSMU(nn.Module):\n",
    "\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size):\n",
    "        super().__init__()\n",
    "\n",
    "        ## DenseNet Composite function: BN -> relu -> 3x3 conv\n",
    "        # 1\n",
    "        self.add_module('norm1', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('smu1', SMU())\n",
    "        self.add_module('conv1', nn.Conv3d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False))\n",
    "\n",
    "        # 2\n",
    "        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n",
    "        self.add_module('smu2', SMU())\n",
    "        self.add_module('conv2', nn.Conv3d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "\n",
    "        #self.memory_efficient = memory_efficient\n",
    "    \n",
    "    def bn_function(self, inputs) -> Tensor:\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.smu1(self.norm1(concated_features)))  # noqa: T484\n",
    "        return bottleneck_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, Tensor):\n",
    "            prev_features = [x]\n",
    "        else:\n",
    "            prev_features = x\n",
    "\n",
    "        bottleneck_output = self.bn_function(prev_features)\n",
    "        new_features = self.conv2(self.smu2(self.norm2(bottleneck_output)))\n",
    "        return new_features  ## **\n",
    "\n",
    "class _DenseBlockSMU(nn.ModuleDict):\n",
    "    # receive and concatenate the outputs of all previous blocks as inputs \n",
    "    # growth rate? the number of channel of feature map in each layer\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayerSMU(num_input_features + i * growth_rate,\n",
    "                                growth_rate, bn_size)\n",
    "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
    "    \n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "class _TransitionSMU(nn.Sequential):\n",
    "    ## convolution + pooling between block\n",
    "    # in paper: bach normalization -> 1x1 conv layer -> 2x2 average pooling layer\n",
    "\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super().__init__()\n",
    "        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('smu', SMU())\n",
    "        self.add_module('conv', nn.Conv3d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n",
    "\n",
    "class DenseNetSMU(nn.Module):\n",
    "    \"\"\"Densenet-BC model class\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subject_data, args,\n",
    "                 n_input_channels=1,conv1_t_size=7,conv1_t_stride=2,no_max_pool=False,\n",
    "                 growth_rate=32,block_config=(6, 12, 24, 16),num_init_features=64,\n",
    "                 bn_size=4,drop_rate=0,num_classes=1000):\n",
    "\n",
    "        super(DenseNetSMU, self).__init__()\n",
    "        self.subject_data = subject_data\n",
    "        self.brain_dtypes = args.data_type\n",
    "        self.cat_target = args.cat_target\n",
    "        self.num_target = args.num_target \n",
    "        self.target = args.cat_target + args.num_target\n",
    "        \n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.conv1_t_size = conv1_t_size\n",
    "        self.conv1_t_stride = conv1_t_stride\n",
    "        self.growth_rate = growth_rate\n",
    "        self.block_config = block_config\n",
    "        self.num_init_features = num_init_features\n",
    "        self.bn_size = bn_size\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "              \n",
    "        self.feature_extractors = self._make_feature_extractors()\n",
    "\n",
    "        # Linear layer\n",
    "        self.FClayers = self._make_fclayers()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_feature_extractors(self):\n",
    "        feature_extractors = []\n",
    "        for brain_dtype in self.brain_dtypes:\n",
    "            # First convolution  \n",
    "            feature_extractor = nn.Sequential(collections.OrderedDict([\n",
    "                ('conv0',nn.Conv3d(self.n_input_channels,\n",
    "                                   self.num_init_features,\n",
    "                                   kernel_size=(self.conv1_t_size, 7, 7),\n",
    "                                   stride=(self.conv1_t_stride, 2, 2),\n",
    "                                   padding=(self.conv1_t_size // 2, 3, 3),\n",
    "                                   bias=False)),\n",
    "                 ('norm0', nn.BatchNorm3d(self.num_init_features)),\n",
    "                 ('smu0', SMU()),\n",
    "                 ('pool0', nn.MaxPool3d(kernel_size=3, stride=2, padding=1))\n",
    "            ]))\n",
    "            # Each denseblock\n",
    "            num_features = self.num_init_features\n",
    "            for i, num_layers in enumerate(self.block_config):\n",
    "                block = _DenseBlockSMU(num_layers=num_layers,\n",
    "                                    num_input_features=num_features,\n",
    "                                    bn_size=self.bn_size,\n",
    "                                    growth_rate=self.growth_rate)\n",
    "                feature_extractor.add_module(f'denseblock{i+1}', block)\n",
    "                num_features = num_features + num_layers*self.growth_rate\n",
    "\n",
    "                if i != len(self.block_config) - 1:\n",
    "                    trans = _TransitionSMU(num_input_features = num_features,\n",
    "                                        num_output_features = num_features // 2)\n",
    "                    feature_extractor.add_module(f'transition{i+1}', trans)\n",
    "                    num_features = num_features // 2\n",
    "\n",
    "            # Final batch norm\n",
    "            feature_extractor.add_module('norm5', nn.BatchNorm3d(num_features))\n",
    "            \n",
    "            feature_extractors.append(feature_extractor)\n",
    "            \n",
    "        self.num_features = num_features\n",
    "        \n",
    "        return nn.ModuleList(feature_extractors)\n",
    "    \n",
    "    def _make_fclayers(self):\n",
    "        FClayer = []\n",
    "        in_dim = self.num_features * len(self.brain_dtypes) # case when output of each CNN is concatenated. Should be modified for other options(i.e. cross attentino MLP)\n",
    "        for cat_label in self.cat_target:\n",
    "            self.out_dim = len(self.subject_data[cat_label].value_counts())                        \n",
    "            FClayer.append(nn.Sequential(nn.Linear(in_dim, self.out_dim)))\n",
    "\n",
    "        for num_label in self.num_target:\n",
    "            FClayer.append(nn.Sequential(nn.Linear(in_dim, 1)))\n",
    "\n",
    "        return nn.ModuleList(FClayer)\n",
    "\n",
    "\n",
    "    def forward(self, images):            \n",
    "        outs = []\n",
    "        results = {'embeddings':[]} if len(self.brain_dtypes) > 1 else {}\n",
    "        for i, x in enumerate(images): # feed each brain modality into its own CNN\n",
    "            features = self.feature_extractors[i](x)\n",
    "            if len(self.brain_dtypes) > 1:\n",
    "                results['embeddings'].append(torch.flatten(features, 1))\n",
    "            out = F.adaptive_avg_pool3d(features, output_size=(1, 1, 1))\n",
    "            out = SMU()(out) # out = F.relu(out, inplace=True) ??? why does this use inplace..?\n",
    "            out = torch.flatten(out, 1)\n",
    "            outs.append(out)\n",
    "            \n",
    "        out = torch.cat(outs,1) # dimension option is important for keeping a shape of (BATCH_SIZE, NUMS OF IMAGE)\n",
    "        for i in range(len(self.FClayers)):\n",
    "            results[self.target[i]] = self.FClayers[i](out)\n",
    "            \n",
    "        return results\n",
    "\n",
    "def generate_model(model_depth, subject_data, args, **kwargs):\n",
    "    assert model_depth in [121, 169, 201, 264]\n",
    "\n",
    "    if model_depth == 121:\n",
    "        model = DenseNetSMU(subject_data, args,\n",
    "                         num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 24, 16),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 169:\n",
    "        model = DenseNetSMU(subject_data, args,\n",
    "                         num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 32, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 201:\n",
    "        model = DenseNetSMU(subject_data, args,\n",
    "                         num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 48, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 264:\n",
    "        model = DenseNetSMU(subject_data, args,\n",
    "                         num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 64, 48),\n",
    "                         **kwargs)\n",
    "    return model\n",
    "\n",
    "def densenetSMU3D121(subject_data, args):\n",
    "    model = generate_model(121, subject_data, args)\n",
    "    return model\n",
    "\n",
    "def densenetSMU3D169(subject_data, args):\n",
    "    model = generate_model(169, subject_data, args)\n",
    "    return model\n",
    "\n",
    "def densenetSMU3D201(subject_data, args):\n",
    "    model = generate_model(201, subject_data, args)\n",
    "    return model\n",
    "\n",
    "def densenetSMU3D264(subject_data, args):\n",
    "    model = generate_model(264, subject_data, args)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1078ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def constrastive_loss(output,metric='cos'):\n",
    "    # << should be implemented later >> case where len(args.data_type) >= 3 \n",
    "    embedding_1, embedding_2 = output['embeddings']\n",
    "    embedding_2_rolled = embedding_2.roll(1,0)\n",
    "    \n",
    "    if metric == 'cos':\n",
    "        label_positive = torch.ones(embedding_1.shape[0]).to('cuda:0')\n",
    "        label_negative = -torch.ones(embedding_1.shape[0]).to('cuda:0')\n",
    "\n",
    "        criterion_ssim = nn.CosineEmbeddingLoss(margin=0.0, reduction='mean')\n",
    "        loss_positive = criterion_ssim(embedding_1, embedding_2, label_positive)\n",
    "        loss_negative = criterion_ssim(embedding_1, embedding_2_rolled, label_negative)\n",
    "    \n",
    "    elif metric == 'L2':\n",
    "        criterion_ssim = nn.MSELoss()\n",
    "        loss_positive = criterion_ssim(embedding_1, embedding_2)\n",
    "        loss_negative = torch.zeros(loss_positive.shape)\n",
    "    \n",
    "    output.pop('embeddings')\n",
    "    \n",
    "    return loss_positive, loss_negative\n",
    "    \n",
    "\n",
    "\n",
    "def calc_acc(tmp_output, label, args, tmp_loss=None):\n",
    "    _, predicted = torch.max(tmp_output.data, 1)\n",
    "    correct = (predicted == label).sum().item()\n",
    "    total = label.size(0)\n",
    "    \n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "def calc_R2(tmp_output, y_true, args, tmp_loss=None):\n",
    "    if ('MAE' in [args.transfer, args.scratch]) or tmp_loss == None:\n",
    "        criterion = nn.MSELoss()\n",
    "        tmp_loss = criterion(tmp_output.float(), y_true.float().unsqueeze(1))\n",
    "\n",
    "    y_var = torch.var(y_true, unbiased=False)\n",
    "    r_square = 1 - (tmp_loss / y_var)\n",
    "                    \n",
    "    return r_square.item()\n",
    "\n",
    "\n",
    "def calculating_loss_acc(targets, output, loss_dict, acc_dict, net, args):\n",
    "    '''define calculating loss and accuracy function used during training and validation step'''\n",
    "    # << should be implemented later >> how to set ssim_weight?\n",
    "    cat_weight = (len(args.cat_target)/(len(args.cat_target)+len(args.num_target)))\n",
    "    num_weight = 1 - cat_weight\n",
    "    loss = 0.0\n",
    "    \n",
    "    # calculate constrastive_loss\n",
    "    if len(args.data_type) > 1:\n",
    "        loss_positive, loss_negative = constrastive_loss(output, args.metric)\n",
    "        loss += (loss_positive + loss_negative)/2\n",
    "        loss_dict['contrastive_loss_positive'].append(loss_positive.item())\n",
    "        loss_dict['contrastive_loss_negative'].append(loss_negative.item())\n",
    "        \n",
    "    # calculate target_losses & accuracies\n",
    "    for curr_target in output:\n",
    "        tmp_output = output[curr_target]\n",
    "        label = targets[curr_target].to('cuda:0')\n",
    "#         label = label.repeat(2)\n",
    "        tmp_label = label.long() if curr_target in args.cat_target else label.float().unsqueeze(1)\n",
    "        weight = cat_weight if curr_target in args.cat_target else num_weight\n",
    "        \n",
    "        if curr_target in args.cat_target:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif curr_target == 'age' and 'MAE' in [args.transfer, args.scratch]:\n",
    "            criterion = nn.L1Loss()\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "        \n",
    "        # Loss\n",
    "        tmp_loss = criterion(tmp_output.float(), tmp_label)\n",
    "        loss += tmp_loss * weight\n",
    "        loss_dict[curr_target].append(tmp_loss.item())\n",
    "        \n",
    "        # Acc\n",
    "        acc_func = calc_acc if curr_target in args.cat_target else calc_R2\n",
    "        acc = acc_func(tmp_output, label, args, tmp_loss)\n",
    "        acc_dict[curr_target].append(acc) \n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab48b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# from envs.loss_functions import calculating_loss_acc, calc_acc, calc_R2\n",
    "\n",
    "### ========= Train,Validate, and Test ========= ###\n",
    "'''The process of calcuating loss and accuracy metrics is as follows.\n",
    "   1) sequentially calculate loss and accuracy metrics of target labels with for loop.\n",
    "   2) store the result information with dictionary type.\n",
    "   3) return the dictionary, which form as {'cat_target':value, 'num_target:value}\n",
    "   This process is intended to easily deal with loss values from each target labels.'''\n",
    "\n",
    "\n",
    "'''All of the loss from predictions are summated and this loss value is used for backpropagation.'''\n",
    "\n",
    "# define training step\n",
    "def train(net,partition,optimizer,args):\n",
    "    def seed_worker(worker_id):\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "    '''GradScaler is for calculating gradient with float 16 type'''\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
    "                                              batch_size=args.train_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=4,\n",
    "                                              worker_init_fn=seed_worker,\n",
    "                                              generator=g)\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_loss = defaultdict(list)\n",
    "    train_acc =  defaultdict(list)\n",
    "\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        optimizer.zero_grad()\n",
    "        image, targets = data\n",
    "        image = list(map(lambda x: x.to(f'cuda:{net.device_ids[0]}'), image))\n",
    "        output = net(image)\n",
    "        loss = calculating_loss_acc(targets, output, train_loss, train_acc, net, args)\n",
    "        \n",
    "        # multi-head model sum all the loss from predicting each target variable and back propagation\n",
    "        scaler.scale(loss).backward() \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # calculating total loss and acc of separate mini-batch\n",
    "    for target in train_loss:\n",
    "        train_loss[target] = np.mean(train_loss[target])\n",
    "        train_acc[target] = np.mean(train_acc[target])\n",
    "\n",
    "    return net, train_loss, train_acc\n",
    "\n",
    "\n",
    "# define validation step\n",
    "def validate(net,partition,scheduler,args):\n",
    "    def seed_worker(worker_id):\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=4,\n",
    "                                            worker_init_fn=seed_worker,\n",
    "                                            generator=g)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    val_loss = defaultdict(list)\n",
    "    val_acc = defaultdict(list)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader,0):\n",
    "            image, targets = data\n",
    "            image = list(map(lambda x: x.to(f'cuda:{net.device_ids[0]}'), image))\n",
    "            output = net(image)\n",
    "            loss = calculating_loss_acc(targets, output, val_loss, val_acc, net, args)\n",
    "\n",
    "    for target in val_loss:\n",
    "        val_loss[target] = np.mean(val_loss[target])\n",
    "        val_acc[target] = np.mean(val_acc[target])\n",
    "\n",
    "    # learning rate scheduler\n",
    "    if scheduler:\n",
    "        if args.scheduler == 'on':\n",
    "            scheduler.step(sum(val_acc.values()))\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def calc_confusion_matrix(confusion_matrices, curr_target, output, y_true):\n",
    "    _, predicted = torch.max(output.data,1)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true.numpy(), predicted.numpy()).ravel()\n",
    "    confusion_matrices[curr_target]['True Positive'] = int(tp)\n",
    "    confusion_matrices[curr_target]['True Negative'] = int(tn)\n",
    "    confusion_matrices[curr_target]['False Positive'] = int(fp)\n",
    "    confusion_matrices[curr_target]['False Negative'] = int(fn) \n",
    "    \n",
    "    \n",
    "# define test step\n",
    "def test(net,partition,args):\n",
    "    def seed_worker(worker_id):\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
    "                                             batch_size=args.test_batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=4,\n",
    "                                             pin_memory=True,\n",
    "                                             worker_init_fn=seed_worker,\n",
    "                                             generator=g)\n",
    "\n",
    "    net.eval()\n",
    "    if hasattr(net, 'module'):\n",
    "        device = net.device_ids[0]\n",
    "    else: \n",
    "        device = 'cuda:0' if args.sbatch =='True' else f'cuda:{args.gpus[0]}'\n",
    "    \n",
    "    outputs = defaultdict(list)\n",
    "    y_true = defaultdict(list)\n",
    "    test_acc = defaultdict(list)\n",
    "    confusion_matrices = defaultdict(defaultdict)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(testloader),0):\n",
    "            image, targets = data\n",
    "            image = list(map(lambda x: x.to(device), image))\n",
    "            output = net(image)\n",
    "            \n",
    "            for curr_target in output:\n",
    "                if curr_target != 'embeddings':\n",
    "                    outputs[curr_target].append(output[curr_target].cpu())\n",
    "                    y_true[curr_target].append(targets[curr_target].cpu())\n",
    "    \n",
    "    # caculating ACC and R2 at once  \n",
    "    for curr_target in output:\n",
    "        if curr_target == 'embeddings':\n",
    "            continue\n",
    "            \n",
    "        outputs[curr_target] = torch.cat(outputs[curr_target])\n",
    "        y_true[curr_target] = torch.cat(y_true[curr_target])\n",
    "        acc_func = calc_acc if curr_target in args.cat_target else calc_R2\n",
    "        curr_acc = acc_func(outputs[curr_target], y_true[curr_target], args, None)\n",
    "        test_acc[curr_target].append(curr_acc)\n",
    "        \n",
    "        if curr_target in args.confusion_matrix:\n",
    "            calc_confusion_matrix(confusion_matrices, curr_target,\n",
    "                                  outputs[curr_target], y_true[curr_target])\n",
    "\n",
    "    return test_acc, confusion_matrices\n",
    "\n",
    "## ============================================ ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5269d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======= load module ======= ##\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import hashlib\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.utils import argument_setting, select_model, CLIreporter, save_exp_result, checkpoint_save, checkpoint_load\n",
    "from dataloaders.dataloaders import make_dataset\n",
    "from dataloaders.preprocessing import preprocessing_cat, preprocessing_num\n",
    "# from envs.experiments import train, validate, test\n",
    "from envs.transfer import setting_transfer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## ========= Helper Functions =============== ##\n",
    "\n",
    "def setup_results(args):\n",
    "    train_losses = defaultdict(list)\n",
    "    train_accs = defaultdict(list)\n",
    "    val_losses = defaultdict(list)\n",
    "    val_accs = defaultdict(list)\n",
    "\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_losses'] = val_losses\n",
    "    result['val_accs'] = val_accs\n",
    "    \n",
    "    return result\n",
    "\n",
    "    \n",
    "def set_optimizer(args, net):\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                              lr=args.lr, momentum=0.9)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                               lr=args.lr, weight_decay=args.weight_decay)\n",
    "    elif args.optim =='RAdam':\n",
    "        optimizer = optim.RAdam(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                                lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'AdamW':\n",
    "        optimizer = optim.AdamW(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                                lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer choice')\n",
    "        \n",
    "    return optimizer\n",
    "    \n",
    "    \n",
    "def set_lr_scheduler(args, optimizer, len_dataloader):\n",
    "    if args.scheduler == '':\n",
    "        scheduler = None\n",
    "    elif args.scheduler == 'on':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'max', patience=10, factor=0.1, min_lr=1e-7)\n",
    "    elif args.scheduler.lower() == 'cos':\n",
    "#             scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=5, T_mult=2, eta_max=0.1, T_up=2, gamma=1)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=0)\n",
    "    elif 'step' in args.scheduler:\n",
    "        step_size = 80 if len(args.scheduler.split('_')) != 2 else int(args.scheduler.split('_')[1])        \n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)\n",
    "    elif args.scheduler.lower() == 'onecycle':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr, total_steps=args.epoch)\n",
    "    else:\n",
    "        raise Exception(\"Invalid scheduler option\")\n",
    "        \n",
    "    return scheduler\n",
    "    \n",
    "    \n",
    "def run_experiment(args, net, partition, result, mode):\n",
    "    epoch_exp = args.epoch if mode == 'ALL' else args.epoch_FC\n",
    "    num_unfrozen = args.unfrozen_layer if mode == 'ALL' else '0'\n",
    "    \n",
    "    if (args.transfer != '') and (args.unfrozen_layer.lower() != 'all'):\n",
    "        setting_transfer(args, net.module, num_unfrozen = num_unfrozen)\n",
    "    optimizer = set_optimizer(args, net)\n",
    "    scheduler = set_lr_scheduler(args, optimizer, len(partition['train']))\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in tqdm(range(epoch_exp)):\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, args)\n",
    "        val_loss, val_acc = validate(net, partition, scheduler, args)\n",
    "        te = time.time()\n",
    "\n",
    "        ## sorting the results\n",
    "        train_loss_sum = 0\n",
    "        val_loss_sum = 0\n",
    "        val_acc_sum = 0\n",
    "        \n",
    "        for target_name in train_loss:\n",
    "            result['train_losses'][target_name].append(train_loss[target_name])\n",
    "            result['val_losses'][target_name].append(val_loss[target_name])\n",
    "            train_loss_sum += train_loss[target_name]\n",
    "            val_loss_sum += val_loss[target_name]\n",
    "            if 'contrastive_loss' not in target_name:\n",
    "                result['train_accs'][target_name].append(train_acc[target_name])\n",
    "                result['val_accs'][target_name].append(val_acc[target_name])\n",
    "                val_acc_sum += val_acc[target_name]\n",
    "            \n",
    "        if val_loss_sum < best_val_loss:\n",
    "            best_val_loss = val_loss_sum\n",
    "            best_train_loss = train_loss_sum\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        ## visualize the result\n",
    "        CLIreporter(train_loss, train_acc, val_loss, val_acc)\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}. Current learning rate {curr_lr}. Took {te-ts:2.2f} sec\")\n",
    "\n",
    "        ## saving the checkpoint and results\n",
    "        if args.debug == '':\n",
    "            checkpoint_dir = checkpoint_save(net, epoch, val_acc, result['val_accs'], args)                     \n",
    "            if epoch%10 == 0:\n",
    "                save_exp_result(vars(args).copy(), result)\n",
    "        else:\n",
    "            checkpoint_dir = None\n",
    "\n",
    "        ## Early-Stopping\n",
    "        if args.early_stopping != None and patience == args.early_stopping:\n",
    "            print(f\"*** Validation Loss patience reached {args.early_stopping} epochs. Early Stopping Experiment ***\")\n",
    "            break\n",
    "    \n",
    "    opt = '' if mode == 'ALL' else '_FC'\n",
    "    result[f'best_val_loss{opt}'] = best_val_loss\n",
    "    result[f'best_train_loss{opt}'] = best_train_loss\n",
    "        \n",
    "    return result, checkpoint_dir\n",
    "\n",
    "\n",
    "## ========= Experiment =============== ##\n",
    "def experiment(partition, subject_data, args):\n",
    "    if args.transfer in ['age','MAE']:\n",
    "        assert 96 in args.resize, \"age(MSE/MAE) transfer model's resize should be 96\"\n",
    "    elif args.transfer == 'sex':\n",
    "        assert 80 in args.resize, \"sex transfer model's resize should be 80\"\n",
    "    \n",
    "    # selecting a model\n",
    "    net = select_model(subject_data, args)\n",
    "#     net = densenetSMU3D121(subject_data, args)\n",
    "    \n",
    "    # loading pretrained model if transfer option is given\n",
    "    if (args.transfer != \"\") and (args.load == \"\"):\n",
    "        print(\"*** Model setting for transfer learning *** \\n\")\n",
    "        net = checkpoint_load(net, args.transfer)\n",
    "    elif args.load:\n",
    "        print(\"*** Model setting for transfer learning & fine tuning *** \\n\")\n",
    "        model_dir = glob.glob(f'/scratch/connectome/jubin/result/model/*{args.load}*')[0]\n",
    "        print(f\"Loaded {model_dir[:-4]}\")\n",
    "        net = checkpoint_load(net, model_dir)\n",
    "    else:\n",
    "        print(\"*** Model setting for learning from scratch ***\")\n",
    "    \n",
    "    # setting a DataParallel and model on GPU\n",
    "    if args.sbatch == \"True\":\n",
    "        devices = []\n",
    "        for d in range(torch.cuda.device_count()):\n",
    "            devices.append(d)\n",
    "        net = nn.DataParallel(net, device_ids = devices)\n",
    "    else:\n",
    "        if not args.gpus:\n",
    "            raise ValueError(\"GPU DEVICE IDS SHOULD BE ASSIGNED\")\n",
    "        else:\n",
    "            net = nn.DataParallel(net, device_ids=args.gpus)\n",
    "            \n",
    "    net.to(f'cuda:{net.device_ids[0]}')\n",
    "    \n",
    "    # setting for results' DataFrame\n",
    "    result = setup_results(args)\n",
    "    \n",
    "    # training a model\n",
    "    print(\"*** Start training a model *** \\n\")\n",
    "    if args.epoch_FC != 0:\n",
    "        print(\"*** Transfer Learning - Training FC layers *** \\n\")\n",
    "        result, _ = run_experiment(args, net, partition, result, 'FC')\n",
    "                \n",
    "        print(f\"Adjust learning rate for Training unfrozen layers from {args.lr} to {args.lr*args.lr_adjust}\")\n",
    "        args.lr *= args.lr_adjust\n",
    "        result['lr_adjusted'] = args.lr\n",
    "            \n",
    "    print(\"*** Training unfrozen layers *** \\n\")\n",
    "    result, checkpoint_dir = run_experiment(args, net, partition, result, 'ALL')\n",
    "                    \n",
    "    # testing a model\n",
    "    if args.debug == '':\n",
    "        print(\"\\n*** Start testing a model *** \\n\")\n",
    "        net.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        net = checkpoint_load(net, checkpoint_dir)\n",
    "        if args.sbatch == 'True':\n",
    "            net.cuda()\n",
    "        else:\n",
    "            net.to(f'cuda:{args.gpus[0]}')\n",
    "        test_acc, confusion_matrices = test(net, partition, args)\n",
    "        result['test_acc'] = test_acc\n",
    "        print(f\"===== Test result for {args.exp_name} =====\") \n",
    "        print(test_acc)\n",
    "\n",
    "        if confusion_matrices != None:\n",
    "            print(\"===== Confusion Matrices =====\")\n",
    "            print(confusion_matrices,'\\n')\n",
    "            result['confusion_matrices'] = confusion_matrices\n",
    "        \n",
    "    return vars(args), result\n",
    "## ==================================== ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f7ee94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/bigdata/ABCD/freesurfer/smri/fs_smri_brain/sub-NDARINVC2ZLETPN.brain.nii.gz\n",
      "sub-NDARINVC2ZLETPN\n",
      "NDARINVC2ZLETPN\n",
      "/scratch/connectome/3DCNN/data/1.ABCD/3.2.FA_warpped_nii/NDARINV89B7M962.nii.gz\n",
      "NDARINV89B7M962\n",
      "NDARINV89B7M962\n",
      "Total subjects=200, train=100, val=50, test=50\n",
      "In train,\t\"sex\" contains 43 CASE and 57 CONTROL\n",
      "In validation,\t\"sex\" contains 22 CASE and 28 CONTROL\n",
      "In test,\t\"sex\" contains 25 CASE and 25 CONTROL\n",
      "*** Making a dataset is completed *** \n",
      "\n",
      "*** Experiment act_func_test1_887d2b Start ***\n",
      "*** Model setting for learning from scratch ***\n",
      "*** Start training a model *** \n",
      "\n",
      "*** Training unfrozen layers *** \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efc02a8c20e40e59277e127e58a37a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0881 / 0.6915                  None\n",
      "contrastive_loss_negative  0.9087 / 0.3085                  None\n",
      "sex                        0.5935 / 0.9089     63.2812 / 45.3125\n",
      "Epoch 1. Current learning rate 0.001. Took 1130.16 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0955 / 0.7518                  None\n",
      "contrastive_loss_negative  0.9011 / 0.2482                  None\n",
      "sex                        0.6780 / 1.1149     60.9375 / 45.3125\n",
      "Epoch 2. Current learning rate 0.001. Took 49.65 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0795 / 0.7846                  None\n",
      "contrastive_loss_negative  0.9126 / 0.2154                  None\n",
      "sex                        0.5861 / 1.1129     74.2188 / 45.3125\n",
      "Epoch 3. Current learning rate 0.001. Took 59.91 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0817 / 0.8042                  None\n",
      "contrastive_loss_negative  0.9084 / 0.1958                  None\n",
      "sex                        0.5306 / 1.0827     74.2188 / 45.3125\n",
      "Epoch 4. Current learning rate 0.001. Took 1467.33 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0806 / 0.8187                  None\n",
      "contrastive_loss_negative  0.9054 / 0.1813                  None\n",
      "sex                        0.4920 / 1.0706     76.5625 / 45.3125\n",
      "Epoch 5. Current learning rate 0.001. Took 48.12 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3253370/3406381093.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Run Experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"*** Experiment {args.exp_name} Start ***\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===== Experiment Setting Report =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3253370/1287378925.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(partition, subject_data, args)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Training unfrozen layers *** \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ALL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# testing a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3253370/1287378925.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(args, net, partition, result, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3253370/3938826513.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(net, partition, scheduler, args)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'cuda:{net.device_ids[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3DCNN/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3DCNN/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3DCNN/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3DCNN/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3DCNN/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3DCNN/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## ========= Setting ========= ##\n",
    "# args = argument_setting()\n",
    "\n",
    "# seed number\n",
    "args.seed = 1234\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.save_dir = os.getcwd() + '/result'\n",
    "partition, subject_data = make_dataset(args)\n",
    "\n",
    "## ========= Run Experiment and saving result ========= ##    \n",
    "time_hash = datetime.datetime.now().time()\n",
    "hash_key = hashlib.sha1(str(time_hash).encode()).hexdigest()[:6]\n",
    "args.exp_name = args.exp_name + f'_{hash_key}'\n",
    "\n",
    "# Run Experiment\n",
    "print(f\"*** Experiment {args.exp_name} Start ***\")\n",
    "setting, result = experiment(partition, subject_data, deepcopy(args))\n",
    "print(\"===== Experiment Setting Report =====\")\n",
    "print(args)\n",
    "\n",
    "# Save result\n",
    "if args.debug == '':\n",
    "    save_exp_result(setting, result)\n",
    "print(\"*** Experiment Done ***\\n\")\n",
    "## ====================================== ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ca1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/bigdata/ABCD/freesurfer/smri/fs_smri_brain/sub-NDARINVC2ZLETPN.brain.nii.gz\n",
      "sub-NDARINVC2ZLETPN\n",
      "NDARINVC2ZLETPN\n",
      "/scratch/connectome/3DCNN/data/1.ABCD/3.2.FA_warpped_nii/NDARINV89B7M962.nii.gz\n",
      "NDARINV89B7M962\n",
      "NDARINV89B7M962\n",
      "Total subjects=200, train=100, val=50, test=50\n",
      "In train,\t\"sex\" contains 43 CASE and 57 CONTROL\n",
      "In validation,\t\"sex\" contains 22 CASE and 28 CONTROL\n",
      "In test,\t\"sex\" contains 25 CASE and 25 CONTROL\n",
      "*** Making a dataset is completed *** \n",
      "\n",
      "*** Experiment act_func_test1_0c109b Start ***\n",
      "*** Model setting for learning from scratch ***\n",
      "*** Start training a model *** \n",
      "\n",
      "*** Training unfrozen layers *** \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2a2aa7af994ab19ebcb8897b7a9912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.3629 / 0.3795                  None\n",
      "contrastive_loss_negative  0.0000 / 0.0000                  None\n",
      "sex                        0.6053 / 0.8213     63.2812 / 45.3125\n",
      "Epoch 1. Current learning rate 0.001. Took 375.17 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.1663 / 0.5343                  None\n",
      "contrastive_loss_negative  0.0000 / 0.0000                  None\n",
      "sex                        0.6816 / 1.0671     54.6875 / 45.3125\n",
      "Epoch 2. Current learning rate 0.001. Took 50.07 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0793 / 0.5555                  None\n",
      "contrastive_loss_negative  0.0000 / 0.0000                  None\n",
      "sex                        0.6005 / 1.0549     69.5312 / 45.3125\n",
      "Epoch 3. Current learning rate 0.001. Took 929.70 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0635 / 0.5844                  None\n",
      "contrastive_loss_negative  0.0000 / 0.0000                  None\n",
      "sex                        0.5422 / 1.0633     79.6875 / 45.3125\n",
      "Epoch 4. Current learning rate 0.001. Took 408.76 sec\n",
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.0623 / 0.6060                  None\n",
      "contrastive_loss_negative  0.0000 / 0.0000                  None\n",
      "sex                        0.5025 / 1.0597     78.1250 / 45.3125\n",
      "Epoch 5. Current learning rate 0.001. Took 57.35 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## ========= Setting ========= ##\n",
    "# args = argument_setting()\n",
    "\n",
    "# seed number\n",
    "args.seed = 1234\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.save_dir = os.getcwd() + '/result'\n",
    "partition, subject_data = make_dataset(args)\n",
    "\n",
    "## ========= Run Experiment and saving result ========= ##    \n",
    "time_hash = datetime.datetime.now().time()\n",
    "hash_key = hashlib.sha1(str(time_hash).encode()).hexdigest()[:6]\n",
    "args.exp_name = args.exp_name + f'_{hash_key}'\n",
    "\n",
    "# Run Experiment\n",
    "print(f\"*** Experiment {args.exp_name} Start ***\")\n",
    "setting, result = experiment(partition, subject_data, deepcopy(args))\n",
    "print(\"===== Experiment Setting Report =====\")\n",
    "print(args)\n",
    "\n",
    "# Save result\n",
    "if args.debug == '':\n",
    "    save_exp_result(setting, result)\n",
    "print(\"*** Experiment Done ***\\n\")\n",
    "## ====================================== ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

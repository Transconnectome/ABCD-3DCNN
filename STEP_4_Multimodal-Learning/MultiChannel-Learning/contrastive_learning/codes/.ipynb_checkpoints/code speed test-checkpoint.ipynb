{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5879e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/connectome/jubin/ABCD-3DCNN/STEP_4_Multimodal-Learning/MultiChannel-Learning/contrastive_learning/codes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('/scratch/connectome/jubin/ABCD-3DCNN/STEP_4_Multimodal-Learning/MultiChannel-Learning/contrastive_learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14dc13f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--debug'], dest='debug', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse \n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Options for model setting\n",
    "parser.add_argument(\"--model\", type=str, required=True, help='Select model. e.g. densenet3D121, sfcn.',\n",
    "                    choices=['simple3D', 'sfcn', 'vgg3D11', 'vgg3D13', 'vgg3D16', 'vgg3D19',\n",
    "                             'resnet3D50', 'resnet3D101', 'resnet3D152',\n",
    "                             'densenet3D121', 'densenet3D169', 'densenet201', 'densenet264'])\n",
    "parser.add_argument(\"--in_channels\", default=1, type=int, help='')\n",
    "\n",
    "# Options for dataset and data type, split ratio, CV, resize, augmentation\n",
    "parser.add_argument(\"--dataset\", type=str, choices=['UKB','ABCD'], required=True, help='Selelct dataset')\n",
    "parser.add_argument(\"--data_type\", nargs='+', type=str, help='Select data type(sMRI, dMRI)',\n",
    "                    choices=['fmriprep', 'freesurfer', 'freesurfer_256', 'FA_unwarpped_nii', 'FA_warpped_nii',\n",
    "                             'MD_unwarpped_nii', 'MD_warpped_nii', 'RD_unwarpped_nii', 'RD_warpped_nii'])\n",
    "parser.add_argument(\"--val_size\", default=0.1, type=float, help='')\n",
    "parser.add_argument(\"--test_size\", default=0.1, type=float, help='')\n",
    "parser.add_argument(\"--cv\", default=None, type=int, choices=[1,2,3,4,5], help=\"option for 5-fold CV. 1~5.\")\n",
    "parser.add_argument(\"--resize\", nargs=\"*\", default=(96, 96, 96), type=int, help='')\n",
    "parser.add_argument(\"--augmentation\", nargs=\"*\", default=[], type=str, choices=['shift','flip'],\n",
    "                    help=\"Data augmentation - [shift, flip] are available\")\n",
    "\n",
    "# Hyperparameters for model training\n",
    "parser.add_argument(\"--lr\", default=0.01, type=float, help='')\n",
    "parser.add_argument(\"--lr_adjust\", default=0.01, type=float, help='')\n",
    "parser.add_argument(\"--epoch\", type=int, required=True, help='')\n",
    "parser.add_argument(\"--epoch_FC\", type=int, default=0, help='Option for training only FC layer')\n",
    "parser.add_argument(\"--optim\", default='Adam', type=str, choices=['Adam','SGD','RAdam','AdamW'], help='')\n",
    "parser.add_argument(\"--weight_decay\", default=0.001, type=float, help='')\n",
    "parser.add_argument(\"--scheduler\", default='', type=str, help='') \n",
    "parser.add_argument(\"--early_stopping\", default=None, type=int, help='')\n",
    "parser.add_argument(\"--train_batch_size\", default=16, type=int, help='')\n",
    "parser.add_argument(\"--val_batch_size\", default=16, type=int, help='')\n",
    "parser.add_argument(\"--test_batch_size\", default=1, type=int, help='')\n",
    "\n",
    "# Options for experiment setting\n",
    "parser.add_argument(\"--exp_name\", type=str, required=True, help='')\n",
    "parser.add_argument(\"--gpus\", nargs='+', type=int, help='')\n",
    "parser.add_argument(\"--sbatch\", type=str, choices=['True', 'False'])\n",
    "parser.add_argument(\"--cat_target\", nargs='+', default=[], type=str, help='')\n",
    "parser.add_argument(\"--metric\", default='cos', type=str, choices=['cos', 'L2'],\n",
    "                    help='Option for similarity loss metric')\n",
    "parser.add_argument(\"--num_target\", nargs='+', default=[], type=str, help='')\n",
    "parser.add_argument(\"--confusion_matrix\",  nargs='*', default=[], type=str, help='')\n",
    "parser.add_argument(\"--filter\", nargs=\"*\", default=[], type=str,\n",
    "                    help='options for filter data by phenotype. usage: --filter abcd_site:10 sex:1')\n",
    "parser.add_argument(\"--load\", default='', type=str, help='Load model weight that mathces {your_exp_dir}/result/*{load}*')\n",
    "parser.add_argument(\"--scratch\", default='', type=str, help='Option for learning from scratch')\n",
    "parser.add_argument(\"--transfer\", default='', type=str, choices=['sex','age','simclr','MAE'],\n",
    "                    help='Choose pretrained model according to your option')\n",
    "parser.add_argument(\"--unfrozen_layer\", default='0', type=str, help='Select the number of layers that would be unfrozen')\n",
    "parser.add_argument(\"--init_unfrozen\", default='', type=str, help='Initializes unfrozen layers')\n",
    "parser.add_argument(\"--debug\", default='', type=str, help='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722d2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"sex\"  #Attention.Deficit.Hyperactivity.Disorder.x\n",
    "data_type=\"freesurfer_256 FA_warpped_nii\"\n",
    "model=\"sfcn\"\n",
    "resize=\"96 96 96\"\n",
    "epoch_FC=\"0\"\n",
    "epoch=\"20\"\n",
    "optim=\"AdamW\"\n",
    "scheduler=\"--scheduler on\" # step_80\"\n",
    "batch=\"16\" \n",
    "val_size=\"0.25\"\n",
    "test_size=\"0.25\"\n",
    "lr=\"1e-3\"\n",
    "lr_adjust=\"--lr_adjust 1\"\n",
    "cfm=\"--confusion_matrix sex\" #--confusion_matrix Attention.Deficit.Hyperactivity.Disorder.x\n",
    "unfrozen=\"all\" \n",
    "weight_decay=\"--weight_decay 0.01\"\n",
    "metric='cos'\n",
    "#early_stopping=\"--early_stopping 20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf96617",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm='''${transfer} --cat_target ${target} --dataset ABCD --data_type ${data_type} --val_size ${val_size} --test_size ${test_size} ${scratch} ${filter} ${aug}\\\n",
    "    --lr ${lr} ${lr_adjust} --optim ${optim} ${weight_decay} --resize ${resize} ${scheduler} --train_batch_size ${batch} --val_batch_size ${batch} ${cfm} ${cv}\\\n",
    "    --exp_name ${exp_name} --model ${model} --epoch ${epoch} --epoch_FC ${epoch_FC} --unfrozen_layer ${unfrozen} --sbatch True ${load} ${early_stopping}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9729047",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{transfer} --cat_target {target} --dataset ABCD --data_type {data_type} --val_size {val_size} --test_size {test_size} {scratch} {filter} {aug}    --lr {lr} {lr_adjust} --optim {optim} {weight_decay} --resize {resize} {scheduler} --train_batch_size {batch} --val_batch_size {batch} {cfm} {cv}    --exp_name {exp_name} --model {model} --epoch {epoch} --epoch_FC {epoch_FC} --unfrozen_layer {unfrozen} --sbatch True {load} {early_stopping}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm = comm.replace(\"$\",'')\n",
    "comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e839c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--cat_target {target} --dataset ABCD --data_type {data_type} --val_size {val_size} --test_size {test_size} --lr {lr} --optim {optim} --resize {resize} --train_batch_size {batch} --val_batch_size {batch} --exp_name {exp_name} --model {model} --epoch {epoch} --epoch_FC {epoch_FC} --unfrozen_layer {unfrozen} --sbatch True'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com = list(filter(lambda x: x != '', comm.split(\" \")))\n",
    "new = []\n",
    "prev=False\n",
    "for c in com.copy():\n",
    "    curr = c.startswith(\"-\")\n",
    "    if curr or prev:\n",
    "        new.append(c)\n",
    "    prev = curr\n",
    "' '.join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83df12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "com = f'--metric cos --cat_target {target} --dataset ABCD --data_type {data_type} --val_size {val_size} --test_size {test_size} \\\n",
    "--lr {lr} --optim {optim} --resize {resize} --train_batch_size {batch} --val_batch_size {batch} \\\n",
    "--exp_name act_func_test1 --model {model} --epoch {epoch} --epoch_FC {epoch_FC} --unfrozen_layer {unfrozen} \\\n",
    "--gpus 0 --debug 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7855cb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--metric cos --cat_target sex --dataset ABCD --data_type freesurfer_256 FA_warpped_nii --val_size 0.25 --test_size 0.25 --lr 1e-3 --optim AdamW --resize 96 96 96 --train_batch_size 16 --val_batch_size 16 --exp_name act_func_test1 --model sfcn --epoch 20 --epoch_FC 0 --unfrozen_layer all --gpus 0 --debug 1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a2353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Categorical target labels are ['sex'] and Numerical target labels are [] *** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(com.split())\n",
    "if args.cat_target == args.num_target:\n",
    "    raise ValueError('--num-target or --cat-target should be specified')\n",
    "\n",
    "print(f\"*** Categorical target labels are {args.cat_target} and Numerical target labels are {args.num_target} *** \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a045a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from monai.transforms import (AddChannel, Compose, CenterSpatialCrop, Flip, RandAffine,\n",
    "                              RandFlip, RandRotate90, Resize, ScaleIntensity, ToTensor)\n",
    "from monai.data import ImageDataset, NibabelReader\n",
    "\n",
    "from dataloaders.custom_dataset import MultiModalImageDataset\n",
    "from dataloaders.preprocessing import preprocessing_cat, preprocessing_num\n",
    "\n",
    "def case_control_count(labels, dataset_type, args):\n",
    "    if args.cat_target:\n",
    "        for cat_target in args.cat_target:\n",
    "            curr_cnt = labels[cat_target].value_counts()\n",
    "            print(f'In {dataset_type},\\t\"{cat_target}\" contains {curr_cnt[1]} CASE and {curr_cnt[0]} CONTROL')\n",
    "            \n",
    "\n",
    "## ========= Define directories of data ========= ##\n",
    "# revising\n",
    "ABCD_data_dir = {\n",
    "    'fmriprep':'/scratch/connectome/3DCNN/data/1.ABCD/1.sMRI_fmriprep/preprocessed_masked/',\n",
    "    'freesurfer':'/scratch/connectome/3DCNN/data/1.ABCD/2.sMRI_freesurfer/',\n",
    "    'freesurfer_256':'/storage/bigdata/ABCD/freesurfer/smri/fs_smri_brain/',\n",
    "    'FA_unwarpped_nii':'/scratch/connectome/3DCNN/data/1.ABCD/3.1.FA_unwarpped_nii/',\n",
    "    'FA_warpped_nii':'/scratch/connectome/3DCNN/data/1.ABCD/3.2.FA_warpped_nii/',\n",
    "    'MD_unwarpped_nii':'/scratch/connectome/3DCNN/data/1.ABCD/3.3.MD_unwarpped_nii/',\n",
    "    'MD_warpped_nii':'/scratch/connectome/3DCNN/data/1.ABCD/3.4.MD_warpped_nii/',\n",
    "    'RD_unwarpped_nii':'/scratch/connectome/3DCNN/data/1.ABCD/3.5.RD_unwarpped_nii/',\n",
    "    'RD_warpped_nii':'/scratch/connectome/3DCNN/data/1.ABCD/3.6.RD_warpped_nii/'\n",
    "}\n",
    "\n",
    "ABCD_phenotype_dir = {\n",
    "    'total':'/scratch/connectome/3DCNN/data/1.ABCD/4.demo_qc/ABCD_phenotype_total.csv',\n",
    "    'ADHD_case':'/scratch/connectome/3DCNN/data/1.ABCD/4.demo_qc/ABCD_ADHD.csv',\n",
    "    'suicide_case':'/scratch/connectome/3DCNN/data/1.ABCD/4.demo_qc/ABCD_suicide_case.csv',\n",
    "    'suicide_control':'/scratch/connectome/3DCNN/data/1.ABCD/4.demo_qc/ABCD_suicide_control.csv'\n",
    "}\n",
    "\n",
    "UKB_data_dir = '/scratch/connectome/3DCNN/data/2.UKB/1.sMRI_fs_cropped/'\n",
    "UKB_phenotype_dir = '/scratch/connectome/3DCNN/data/2.UKB/2.demo_qc/UKB_phenotype.csv'\n",
    "\n",
    "\n",
    "## ========= Define helper functions ========= ##\n",
    "\n",
    "def loading_images(image_dir, args):\n",
    "    image_files = pd.DataFrame()\n",
    "    for brain_modality in args.data_type:\n",
    "        curr_dir = image_dir[brain_modality]\n",
    "        curr_files = pd.DataFrame({brain_modality:glob.glob(curr_dir+'*[yz]')}) # to get .npy(sMRI) & .nii.gz(dMRI) files\n",
    "        curr_files[subjectkey] = curr_files[brain_modality].map(lambda x: x.split(\"/\")[-1].split('.')[0])\n",
    "        if 'sub' in curr_files[subjectkey][0]:\n",
    "            curr_files[subjectkey] = curr_files[subjectkey].map(lambda x: x.split(\"-\")[1])\n",
    "        if args.dataset == 'UKB':\n",
    "            curr_files[subjectkey] = curr_files[subjectkey].map(int)\n",
    "            \n",
    "        curr_files.sort_values(by=subjectkey, inplace=True)\n",
    "        if len(image_files) == 0:\n",
    "            image_files = curr_files\n",
    "        else:\n",
    "            image_files = pd.merge(image_files, curr_files, how='inner', on=subjectkey)\n",
    "            \n",
    "    if args.debug:\n",
    "        image_files = image_files[:200]\n",
    "        \n",
    "    return image_files\n",
    "\n",
    "\n",
    "def get_available_subjects(subject_data, args):\n",
    "    case  = pd.read_csv(ABCD_phenotype_dir['ADHD_case'])[subjectkey]\n",
    "    control = pd.read_csv(ABCD_phenotype_dir['suicide_control'])[subjectkey]\n",
    "    filtered_subjectkey = pd.concat([case,control]).reset_index(drop=True)\n",
    "    subject_data = subject_data[subject_data[subjectkey].isin(filtered_subjectkey)]\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "\n",
    "def filter_phenotype(subject_data, filters):\n",
    "    for fil in filters:\n",
    "        fil_name, fil_option = fil.split(':')\n",
    "        fil_option = np.float64(fil_option)\n",
    "        subject_data = subject_data[subject_data[fil_name] == fil_option]\n",
    "        \n",
    "    return subject_data\n",
    "\n",
    "\n",
    "def loading_phenotype(phenotype_dir, target_list, args):\n",
    "    col_list = target_list + [subjectkey]\n",
    "\n",
    "    ## get subject ID and target variables\n",
    "    subject_data = pd.read_csv(phenotype_dir)\n",
    "    subject_data = subject_data.loc[:,col_list]\n",
    "    if 'Attention.Deficit.Hyperactivity.Disorder.x' in target_list:\n",
    "        subject_data = get_available_subjects(subject_data, args)\n",
    "    subject_data = filter_phenotype(subject_data, args.filter)\n",
    "    subject_data = subject_data.sort_values(by=subjectkey)\n",
    "    subject_data = subject_data.dropna(axis = 0)\n",
    "    subject_data = subject_data.reset_index(drop=True)\n",
    "    \n",
    "    if (args.transfer == 'MAE' and args.dataset == 'ABCD') or args.scratch == 'MAE':\n",
    "        return subject_data\n",
    "\n",
    "    ### preprocessing categorical variables and numerical variables\n",
    "    subject_data = preprocessing_cat(subject_data, args)\n",
    "    subject_data = preprocessing_num(subject_data, args)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "\n",
    "def make_balanced_testset(imageFiles_labels, num_test, args):\n",
    "    il = imageFiles_labels\n",
    "    n_case = num_test//2\n",
    "    n_control = num_test - n_case\n",
    "\n",
    "    # << should be implemented later >> code for multiple categorical target\n",
    "    t_case, rest_case = np.split(il[il[args.cat_target[0]]==0], (n_case,)) \n",
    "    t_control, rest_control = np.split(il[il[args.cat_target[0]]==1],(n_control,))\n",
    "    \n",
    "    test = pd.concat((t_case, t_control))\n",
    "    rest = pd.concat((rest_case, rest_control))\n",
    "    \n",
    "    test = test.sort_values(by=subjectkey)\n",
    "    rest = rest.sort_values(by=subjectkey)\n",
    "    \n",
    "    imageFiles_labels = pd.concat((rest,test)).reset_index(drop=True)\n",
    "    \n",
    "    return imageFiles_labels\n",
    "\n",
    "\n",
    "# defining train,val, test set splitting function\n",
    "def partition_dataset(imageFiles_labels, target_list, args):\n",
    "    ## Define transform function\n",
    "    resize = tuple(args.resize)\n",
    "    \n",
    "    default_transforms = [ScaleIntensity(), AddChannel(), Resize(resize), ToTensor()] #AddChannel will be deprecated\n",
    "    dMRI_transform = [CenterSpatialCrop(192)] + default_transforms\n",
    "    aug_transforms = []\n",
    "    \n",
    "    if 'shift' in args.augmentation:\n",
    "        aug_transforms.append(RandAffine(prob=0.1,translate_range=(0,2),padding_mode='zeros'))\n",
    "    elif 'flip' in args.augmentation:\n",
    "        aug_transforms.append(RandFlip(prob=0.1, spatial_axis=0))\n",
    "    \n",
    "    train_transforms, val_transforms, test_transforms = [], [], []\n",
    "    for brain_modality in args.data_type:\n",
    "        if (re.search('FA|MD|RD',brain_modality) != None) or (brain_modality == 'freesurfer_256'):\n",
    "            train_transforms.append(Compose(dMRI_transform+aug_transforms))\n",
    "            val_transforms.append(Compose(dMRI_transform))\n",
    "            test_transforms.append(Compose(dMRI_transform))\n",
    "        else:\n",
    "            train_transforms.append(Compose(default_transforms+aug_transforms))\n",
    "            val_transforms.append(Compose(default_transforms))\n",
    "            test_transforms.append(Compose(default_transforms))\n",
    "\n",
    "    ## Dataset split\n",
    "    num_total = len(imageFiles_labels)\n",
    "    num_test = int(num_total*args.test_size)\n",
    "    num_val = int(num_total*args.val_size) if args.cv == None else int((num_total-num_test)/5)\n",
    "    num_train = num_total - (num_val+num_test)\n",
    "    \n",
    "    imageFiles_labels = make_balanced_testset(imageFiles_labels, num_test, args)\n",
    "    images = imageFiles_labels[args.data_type]\n",
    "    labels = imageFiles_labels[target_list]\n",
    "    \n",
    "    ## split dataset by 5-fold cv or given split size\n",
    "    if args.cv == None:\n",
    "        images_train, images_val, images_test = np.split(images, [num_train, num_train+num_val]) # revising\n",
    "        labels_train, labels_val, labels_test = np.split(labels, [num_train, num_train+num_val])\n",
    "    else:\n",
    "        split_points = [num_val, 2*num_val, 3*num_val, 4*num_val, num_total-num_test]\n",
    "        images_total, labels_total = np.split(images, split_points), np.split(labels, split_points)\n",
    "        images_test, labels_test = images_total.pop(), labels_total.pop()\n",
    "        images_val, labels_val = images_total.pop(args.cv-1), labels_total.pop(args.cv-1)\n",
    "        images_train, labels_train = pd.concat(images_total), pd.concat(labels_total)\n",
    "        num_train, num_val = images_train.shape[0], images_val.shape[0]\n",
    "        \n",
    "    print(f\"Total subjects={num_total}, train={num_train}, val={num_val}, test={num_test}\")\n",
    "\n",
    "    ## make splitted dataset\n",
    "    train_set = MultiModalImageDataset(image_files=images_train, labels=labels_train, transform=train_transforms)\n",
    "    val_set = MultiModalImageDataset(image_files=images_val, labels=labels_val, transform=val_transforms)\n",
    "    test_set = MultiModalImageDataset(image_files=images_test, labels=labels_test, transform=test_transforms)\n",
    "\n",
    "    partition = {}\n",
    "    partition['train'] = train_set\n",
    "    partition['val'] = val_set\n",
    "    partition['test'] = test_set\n",
    "\n",
    "    case_control_count(labels_train, 'train', args)\n",
    "    case_control_count(labels_val, 'validation', args)\n",
    "    case_control_count(labels_test, 'test', args)\n",
    "\n",
    "    return partition\n",
    "## ====================================== ##\n",
    "\n",
    "\n",
    "## ========= Main function that makes partition of dataset  ========= ##\n",
    "def make_dataset(args): # revising\n",
    "    global subjectkey\n",
    "    subjectkey = 'subjectkey' if args.dataset == 'ABCD' else 'eid'\n",
    "    image_dir = ABCD_data_dir if args.dataset == 'ABCD' else UKB_data_dir\n",
    "    phenotype_dir = ABCD_phenotype_dir['total'] if args.dataset == 'ABCD' else UKB_phenotype_dir\n",
    "    target_list = args.cat_target + args.num_target\n",
    "    \n",
    "    image_files = loading_images(image_dir, args)\n",
    "    subject_data = loading_phenotype(phenotype_dir, target_list, args)\n",
    "\n",
    "    # combining image files & labels\n",
    "    imageFiles_labels = pd.merge(subject_data, image_files, how='inner', on=subjectkey)\n",
    "\n",
    "    # partitioning dataset and preprocessing (change the range of categorical variables and standardize numerical variables)\n",
    "    partition = partition_dataset(imageFiles_labels, target_list, args)\n",
    "    print(\"*** Making a dataset is completed *** \\n\")\n",
    "    \n",
    "    return partition, subject_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1078ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "def constrastive_loss(output,metric='cos'):\n",
    "    # << should be implemented later >> case where len(args.data_type) >= 3 \n",
    "    embedding_1, embedding_2 = output['embeddings']\n",
    "    ts = time.time()\n",
    "    embedding_2_rolled = embedding_2.roll(1,0)\n",
    "    time_roll = time.time()\n",
    "    print('roll', time_roll - ts)\n",
    "    \n",
    "    if metric == 'cos':\n",
    "        label_positive = torch.ones(embedding_1.shape[0]).to('cuda:0')\n",
    "        label_negative = -torch.ones(embedding_1.shape[0]).to('cuda:0')\n",
    "        time_make_label = time.time()\n",
    "        print('make label',time_make_label-time_roll)\n",
    "        criterion_ssim = nn.CosineEmbeddingLoss(margin=0.0, reduction='mean')\n",
    "        loss_positive = criterion_ssim(embedding_1, embedding_2, label_positive)\n",
    "        loss_negative = criterion_ssim(embedding_1, embedding_2_rolled, label_negative)\n",
    "        time_ssimloss = time.time()\n",
    "        print('ssim loss', time_ssimloss - time_make_label)\n",
    "    \n",
    "    elif metric == 'L2':\n",
    "        criterion_ssim = nn.MSELoss()\n",
    "        loss_positive = criterion_ssim(embedding_1, embedding_2)\n",
    "        loss_negative = torch.zeros(loss_positive.shape)\n",
    "    \n",
    "    output.pop('embeddings')\n",
    "    time_pop = time.time()\n",
    "    print('pop time', time_pop - time_ssimloss)\n",
    "    return loss_positive, loss_negative\n",
    "    \n",
    "\n",
    "\n",
    "def calc_acc(tmp_output, label, args, tmp_loss=None):\n",
    "    time_acc = time.time()\n",
    "    _, predicted = torch.max(tmp_output.data, 1)\n",
    "    correct = (predicted == label).sum().item()\n",
    "    total = label.size(0)\n",
    "    print('time calc acc',time.time() - time_acc)\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "def calc_R2(tmp_output, y_true, args, tmp_loss=None):\n",
    "    time_R2 = time.time()\n",
    "    if ('MAE' in [args.transfer, args.scratch]) or tmp_loss == None:\n",
    "        criterion = nn.MSELoss()\n",
    "        tmp_loss = criterion(tmp_output.float(), y_true.float().unsqueeze(1))\n",
    "\n",
    "    y_var = torch.var(y_true, unbiased=False)\n",
    "    r_square = 1 - (tmp_loss / y_var)\n",
    "    print('time calc R2',time.time() - time_R2)\n",
    "    return r_square.item()\n",
    "\n",
    "\n",
    "def calculating_loss_acc(targets, output, loss_dict, acc_dict, net, args):\n",
    "    '''define calculating loss and accuracy function used during training and validation step'''\n",
    "    # << should be implemented later >> how to set ssim_weight?\n",
    "    cat_weight = (len(args.cat_target)/(len(args.cat_target)+len(args.num_target)))\n",
    "    num_weight = 1 - cat_weight\n",
    "    loss = 0.0\n",
    "    time_loss = time.time()\n",
    "    # calculate constrastive_loss\n",
    "    if len(args.data_type) > 1:\n",
    "        loss_positive, loss_negative = constrastive_loss(output, args.metric)\n",
    "        loss += (loss_positive + loss_negative)/2\n",
    "        loss_dict['contrastive_loss_positive'].append(loss_positive.item())\n",
    "        loss_dict['contrastive_loss_negative'].append(loss_negative.item())\n",
    "    time_cl = time.time()\n",
    "    print('contrastive loss time',time_cl - time.time())\n",
    "    # calculate target_losses & accuracies\n",
    "    for curr_target in output:\n",
    "        tmp_output = output[curr_target]\n",
    "        label = targets[curr_target].to('cuda:0')\n",
    "#         label = label.repeat(2)\n",
    "        tmp_label = label.long() if curr_target in args.cat_target else label.float().unsqueeze(1)\n",
    "        weight = cat_weight if curr_target in args.cat_target else num_weight\n",
    "        \n",
    "        if curr_target in args.cat_target:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif curr_target == 'age' and 'MAE' in [args.transfer, args.scratch]:\n",
    "            criterion = nn.L1Loss()\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "        \n",
    "        # Loss\n",
    "        tmp_loss = criterion(tmp_output.float(), tmp_label)\n",
    "        loss += tmp_loss * weight\n",
    "        loss_dict[curr_target].append(tmp_loss.item())\n",
    "        \n",
    "        # Acc\n",
    "        acc_func = calc_acc if curr_target in args.cat_target else calc_R2\n",
    "        acc = acc_func(tmp_output, label, args, tmp_loss)\n",
    "        acc_dict[curr_target].append(acc) \n",
    "    print(\"target loss time\",time.time() - time_cl)        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab48b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# from envs.loss_functions import calculating_loss_acc, calc_acc, calc_R2\n",
    "\n",
    "### ========= Train,Validate, and Test ========= ###\n",
    "'''The process of calcuating loss and accuracy metrics is as follows.\n",
    "   1) sequentially calculate loss and accuracy metrics of target labels with for loop.\n",
    "   2) store the result information with dictionary type.\n",
    "   3) return the dictionary, which form as {'cat_target':value, 'num_target:value}\n",
    "   This process is intended to easily deal with loss values from each target labels.'''\n",
    "\n",
    "\n",
    "'''All of the loss from predictions are summated and this loss value is used for backpropagation.'''\n",
    "\n",
    "# define training step\n",
    "def train(net,partition,optimizer,args):\n",
    "    print(\"train start\")\n",
    "    ts = time.time()\n",
    "    def seed_worker(worker_id):\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "    '''GradScaler is for calculating gradient with float 16 type'''\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
    "                                              batch_size=args.train_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=4,\n",
    "                                              worker_init_fn=seed_worker,\n",
    "                                              generator=g)\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_loss = defaultdict(list)\n",
    "    train_acc =  defaultdict(list)\n",
    "    \n",
    "    time_iter_prev = time.time()\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        time_iter_curr = time.time()\n",
    "        print('iter start',time_iter_curr - time_iter_prev)\n",
    "        optimizer.zero_grad()\n",
    "        image, targets = data\n",
    "        time_iter_getdata = time.time()\n",
    "        print('iter get data', time_iter_getdata - time_iter_curr)\n",
    "        image = list(map(lambda x: x.to(f'cuda:{net.device_ids[0]}'), image))\n",
    "        time_iter_img2gpu = time.time()\n",
    "        print('iter img2gpu', time_iter_img2gpu - time_iter_getdata)\n",
    "        output = net(image)\n",
    "        time_output = time.time()\n",
    "        print('iter output',  time_output- time_iter_img2gpu)\n",
    "        \n",
    "        loss = calculating_loss_acc(targets, output, train_loss, train_acc, net, args)\n",
    "        time_loss = time.time()\n",
    "        print('iter loss', time_loss - time_output)\n",
    "        \n",
    "        # multi-head model sum all the loss from predicting each target variable and back propagation\n",
    "        scaler.scale(loss).backward() \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        time_scaler = time_iter_prev = time.time()\n",
    "        print('iter scaler', time_scaler - time_loss)\n",
    "\n",
    "    # calculating total loss and acc of separate mini-batch\n",
    "    time_mean_loss = time.time()\n",
    "    for target in train_loss:\n",
    "        train_loss[target] = np.mean(train_loss[target])\n",
    "        train_acc[target] = np.mean(train_acc[target])\n",
    "    te = time.time()\n",
    "    print('mean loss',te-time_mean_loss)\n",
    "    \n",
    "    print('train time',te - ts)\n",
    "    return net, train_loss, train_acc\n",
    "\n",
    "\n",
    "# define validation step\n",
    "def validate(net,partition,scheduler,args):\n",
    "    def seed_worker(worker_id):\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=4,\n",
    "                                            worker_init_fn=seed_worker,\n",
    "                                            generator=g)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    val_loss = defaultdict(list)\n",
    "    val_acc = defaultdict(list)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader,0):\n",
    "            image, targets = data\n",
    "            image = list(map(lambda x: x.to(f'cuda:{net.device_ids[0]}'), image))\n",
    "            output = net(image)\n",
    "            loss = calculating_loss_acc(targets, output, val_loss, val_acc, net, args)\n",
    "\n",
    "    for target in val_loss:\n",
    "        val_loss[target] = np.mean(val_loss[target])\n",
    "        val_acc[target] = np.mean(val_acc[target])\n",
    "\n",
    "    # learning rate scheduler\n",
    "    if scheduler:\n",
    "        if args.scheduler == 'on':\n",
    "            scheduler.step(sum(val_acc.values()))\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def calc_confusion_matrix(confusion_matrices, curr_target, output, y_true):\n",
    "    _, predicted = torch.max(output.data,1)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true.numpy(), predicted.numpy()).ravel()\n",
    "    confusion_matrices[curr_target]['True Positive'] = int(tp)\n",
    "    confusion_matrices[curr_target]['True Negative'] = int(tn)\n",
    "    confusion_matrices[curr_target]['False Positive'] = int(fp)\n",
    "    confusion_matrices[curr_target]['False Negative'] = int(fn) \n",
    "    \n",
    "    \n",
    "# define test step\n",
    "def test(net,partition,args):\n",
    "    def seed_worker(worker_id):\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
    "                                             batch_size=args.test_batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=4,\n",
    "                                             pin_memory=True,\n",
    "                                             worker_init_fn=seed_worker,\n",
    "                                             generator=g)\n",
    "\n",
    "    net.eval()\n",
    "    if hasattr(net, 'module'):\n",
    "        device = net.device_ids[0]\n",
    "    else: \n",
    "        device = 'cuda:0' if args.sbatch =='True' else f'cuda:{args.gpus[0]}'\n",
    "    \n",
    "    outputs = defaultdict(list)\n",
    "    y_true = defaultdict(list)\n",
    "    test_acc = defaultdict(list)\n",
    "    confusion_matrices = defaultdict(defaultdict)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(testloader),0):\n",
    "            image, targets = data\n",
    "            image = list(map(lambda x: x.to(device), image))\n",
    "            output = net(image)\n",
    "            \n",
    "            for curr_target in output:\n",
    "                if curr_target != 'embeddings':\n",
    "                    outputs[curr_target].append(output[curr_target].cpu())\n",
    "                    y_true[curr_target].append(targets[curr_target].cpu())\n",
    "    \n",
    "    # caculating ACC and R2 at once  \n",
    "    for curr_target in output:\n",
    "        if curr_target == 'embeddings':\n",
    "            continue\n",
    "            \n",
    "        outputs[curr_target] = torch.cat(outputs[curr_target])\n",
    "        y_true[curr_target] = torch.cat(y_true[curr_target])\n",
    "        acc_func = calc_acc if curr_target in args.cat_target else calc_R2\n",
    "        curr_acc = acc_func(outputs[curr_target], y_true[curr_target], args, None)\n",
    "        test_acc[curr_target].append(curr_acc)\n",
    "        \n",
    "        if curr_target in args.confusion_matrix:\n",
    "            calc_confusion_matrix(confusion_matrices, curr_target,\n",
    "                                  outputs[curr_target], y_true[curr_target])\n",
    "\n",
    "    return test_acc, confusion_matrices\n",
    "\n",
    "## ============================================ ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5269d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======= load module ======= ##\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import hashlib\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.utils import argument_setting, select_model, CLIreporter, save_exp_result, checkpoint_save, checkpoint_load\n",
    "from dataloaders.preprocessing import preprocessing_cat, preprocessing_num\n",
    "# from envs.experiments import train, validate, test\n",
    "from envs.transfer import setting_transfer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## ========= Helper Functions =============== ##\n",
    "\n",
    "def setup_results(args):\n",
    "    train_losses = defaultdict(list)\n",
    "    train_accs = defaultdict(list)\n",
    "    val_losses = defaultdict(list)\n",
    "    val_accs = defaultdict(list)\n",
    "\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_losses'] = val_losses\n",
    "    result['val_accs'] = val_accs\n",
    "    \n",
    "    return result\n",
    "\n",
    "    \n",
    "def set_optimizer(args, net):\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                              lr=args.lr, momentum=0.9)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                               lr=args.lr, weight_decay=args.weight_decay)\n",
    "    elif args.optim =='RAdam':\n",
    "        optimizer = optim.RAdam(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                                lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'AdamW':\n",
    "        optimizer = optim.AdamW(params = filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                                lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer choice')\n",
    "        \n",
    "    return optimizer\n",
    "    \n",
    "    \n",
    "def set_lr_scheduler(args, optimizer, len_dataloader):\n",
    "    if args.scheduler == '':\n",
    "        scheduler = None\n",
    "    elif args.scheduler == 'on':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'max', patience=10, factor=0.1, min_lr=1e-7)\n",
    "    elif args.scheduler.lower() == 'cos':\n",
    "#             scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=5, T_mult=2, eta_max=0.1, T_up=2, gamma=1)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=0)\n",
    "    elif 'step' in args.scheduler:\n",
    "        step_size = 80 if len(args.scheduler.split('_')) != 2 else int(args.scheduler.split('_')[1])        \n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)\n",
    "    elif args.scheduler.lower() == 'onecycle':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr, total_steps=args.epoch)\n",
    "    else:\n",
    "        raise Exception(\"Invalid scheduler option\")\n",
    "        \n",
    "    return scheduler\n",
    "    \n",
    "    \n",
    "def run_experiment(args, net, partition, result, mode):\n",
    "    epoch_exp = args.epoch if mode == 'ALL' else args.epoch_FC\n",
    "    num_unfrozen = args.unfrozen_layer if mode == 'ALL' else '0'\n",
    "    \n",
    "    if (args.transfer != '') and (args.unfrozen_layer.lower() != 'all'):\n",
    "        setting_transfer(args, net.module, num_unfrozen = num_unfrozen)\n",
    "    optimizer = set_optimizer(args, net)\n",
    "    scheduler = set_lr_scheduler(args, optimizer, len(partition['train']))\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in tqdm(range(epoch_exp)):\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, args)\n",
    "        val_loss, val_acc = validate(net, partition, scheduler, args)\n",
    "        te = time.time()\n",
    "\n",
    "        ## sorting the results\n",
    "        train_loss_sum = 0\n",
    "        val_loss_sum = 0\n",
    "        val_acc_sum = 0\n",
    "        \n",
    "        for target_name in train_loss:\n",
    "            result['train_losses'][target_name].append(train_loss[target_name])\n",
    "            result['val_losses'][target_name].append(val_loss[target_name])\n",
    "            train_loss_sum += train_loss[target_name]\n",
    "            val_loss_sum += val_loss[target_name]\n",
    "            if 'contrastive_loss' not in target_name:\n",
    "                result['train_accs'][target_name].append(train_acc[target_name])\n",
    "                result['val_accs'][target_name].append(val_acc[target_name])\n",
    "                val_acc_sum += val_acc[target_name]\n",
    "            \n",
    "        if val_loss_sum < best_val_loss:\n",
    "            best_val_loss = val_loss_sum\n",
    "            best_train_loss = train_loss_sum\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        ## visualize the result\n",
    "        CLIreporter(train_loss, train_acc, val_loss, val_acc)\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}. Current learning rate {curr_lr}. Took {te-ts:2.2f} sec\")\n",
    "\n",
    "        ## saving the checkpoint and results\n",
    "        if args.debug == '':\n",
    "            checkpoint_dir = checkpoint_save(net, epoch, val_acc, result['val_accs'], args)                     \n",
    "            if epoch%10 == 0:\n",
    "                save_exp_result(vars(args).copy(), result)\n",
    "        else:\n",
    "            checkpoint_dir = None\n",
    "\n",
    "        ## Early-Stopping\n",
    "        if args.early_stopping != None and patience == args.early_stopping:\n",
    "            print(f\"*** Validation Loss patience reached {args.early_stopping} epochs. Early Stopping Experiment ***\")\n",
    "            break\n",
    "    \n",
    "    opt = '' if mode == 'ALL' else '_FC'\n",
    "    result[f'best_val_loss{opt}'] = best_val_loss\n",
    "    result[f'best_train_loss{opt}'] = best_train_loss\n",
    "        \n",
    "    return result, checkpoint_dir\n",
    "\n",
    "\n",
    "## ========= Experiment =============== ##\n",
    "def experiment(partition, subject_data, args):\n",
    "    if args.transfer in ['age','MAE']:\n",
    "        assert 96 in args.resize, \"age(MSE/MAE) transfer model's resize should be 96\"\n",
    "    elif args.transfer == 'sex':\n",
    "        assert 80 in args.resize, \"sex transfer model's resize should be 80\"\n",
    "    \n",
    "    # selecting a model\n",
    "    net = select_model(subject_data, args)\n",
    "#     net = densenetSMU3D121(subject_data, args)\n",
    "    \n",
    "    # loading pretrained model if transfer option is given\n",
    "    if (args.transfer != \"\") and (args.load == \"\"):\n",
    "        print(\"*** Model setting for transfer learning *** \\n\")\n",
    "        net = checkpoint_load(net, args.transfer)\n",
    "    elif args.load:\n",
    "        print(\"*** Model setting for transfer learning & fine tuning *** \\n\")\n",
    "        model_dir = glob.glob(f'/scratch/connectome/jubin/result/model/*{args.load}*')[0]\n",
    "        print(f\"Loaded {model_dir[:-4]}\")\n",
    "        net = checkpoint_load(net, model_dir)\n",
    "    else:\n",
    "        print(\"*** Model setting for learning from scratch ***\")\n",
    "    \n",
    "    # setting a DataParallel and model on GPU\n",
    "    if args.sbatch == \"True\":\n",
    "        devices = []\n",
    "        for d in range(torch.cuda.device_count()):\n",
    "            devices.append(d)\n",
    "        net = nn.DataParallel(net, device_ids = devices)\n",
    "    else:\n",
    "        if not args.gpus:\n",
    "            raise ValueError(\"GPU DEVICE IDS SHOULD BE ASSIGNED\")\n",
    "        else:\n",
    "            net = nn.DataParallel(net, device_ids=args.gpus)\n",
    "            \n",
    "    net.to(f'cuda:{net.device_ids[0]}')\n",
    "    \n",
    "    # setting for results' DataFrame\n",
    "    result = setup_results(args)\n",
    "    \n",
    "    # training a model\n",
    "    print(\"*** Start training a model *** \\n\")\n",
    "    if args.epoch_FC != 0:\n",
    "        print(\"*** Transfer Learning - Training FC layers *** \\n\")\n",
    "        result, _ = run_experiment(args, net, partition, result, 'FC')\n",
    "                \n",
    "        print(f\"Adjust learning rate for Training unfrozen layers from {args.lr} to {args.lr*args.lr_adjust}\")\n",
    "        args.lr *= args.lr_adjust\n",
    "        result['lr_adjusted'] = args.lr\n",
    "            \n",
    "    print(\"*** Training unfrozen layers *** \\n\")\n",
    "    result, checkpoint_dir = run_experiment(args, net, partition, result, 'ALL')\n",
    "                    \n",
    "    # testing a model\n",
    "    if args.debug == '':\n",
    "        print(\"\\n*** Start testing a model *** \\n\")\n",
    "        net.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        net = checkpoint_load(net, checkpoint_dir)\n",
    "        if args.sbatch == 'True':\n",
    "            net.cuda()\n",
    "        else:\n",
    "            net.to(f'cuda:{args.gpus[0]}')\n",
    "        test_acc, confusion_matrices = test(net, partition, args)\n",
    "        result['test_acc'] = test_acc\n",
    "        print(f\"===== Test result for {args.exp_name} =====\") \n",
    "        print(test_acc)\n",
    "\n",
    "        if confusion_matrices != None:\n",
    "            print(\"===== Confusion Matrices =====\")\n",
    "            print(confusion_matrices,'\\n')\n",
    "            result['confusion_matrices'] = confusion_matrices\n",
    "        \n",
    "    return vars(args), result\n",
    "## ==================================== ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f7ee94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects=200, train=100, val=50, test=50\n",
      "In train,\t\"sex\" contains 43 CASE and 57 CONTROL\n",
      "In validation,\t\"sex\" contains 22 CASE and 28 CONTROL\n",
      "In test,\t\"sex\" contains 25 CASE and 25 CONTROL\n",
      "*** Making a dataset is completed *** \n",
      "\n",
      "*** Experiment act_func_test1_e81f9f Start ***\n",
      "*** Model setting for learning from scratch ***\n",
      "*** Start training a model *** \n",
      "\n",
      "*** Training unfrozen layers *** \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start\n",
      "iter start 18.188964366912842\n",
      "iter get data 0.00034880638122558594\n",
      "iter img2gpu 0.02187061309814453\n",
      "iter output 2.8781778812408447\n",
      "roll 0.0003669261932373047\n",
      "make label 0.052667856216430664\n",
      "ssim loss 0.007183074951171875\n",
      "pop time 8.440017700195312e-05\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.0013835430145263672\n",
      "target loss time 0.006909608840942383\n",
      "iter loss 0.0680696964263916\n",
      "iter scaler 0.2864806652069092\n",
      "iter start 0.0006535053253173828\n",
      "iter get data 0.0009281635284423828\n",
      "iter img2gpu 0.009620189666748047\n",
      "iter output 0.025496482849121094\n",
      "roll 6.0558319091796875e-05\n",
      "make label 0.09604310989379883\n",
      "ssim loss 0.0013747215270996094\n",
      "pop time 0.0004107952117919922\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.0001442432403564453\n",
      "target loss time 0.001081705093383789\n",
      "iter loss 0.10032916069030762\n",
      "iter scaler 0.23676514625549316\n",
      "iter start 0.0004315376281738281\n",
      "iter get data 0.000507354736328125\n",
      "iter img2gpu 0.009308815002441406\n",
      "iter output 0.022803306579589844\n",
      "roll 0.00020241737365722656\n",
      "make label 0.09867167472839355\n",
      "ssim loss 0.0007669925689697266\n",
      "pop time 9.107589721679688e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.893013000488281e-05\n",
      "target loss time 0.0005261898040771484\n",
      "iter loss 0.10085225105285645\n",
      "iter scaler 0.23553776741027832\n",
      "iter start 0.0004343986511230469\n",
      "iter get data 0.0005054473876953125\n",
      "iter img2gpu 0.009320497512817383\n",
      "iter output 0.022975921630859375\n",
      "roll 0.0002090930938720703\n",
      "make label 0.0985417366027832\n",
      "ssim loss 0.0007479190826416016\n",
      "pop time 5.984306335449219e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 9.131431579589844e-05\n",
      "target loss time 0.0005295276641845703\n",
      "iter loss 0.10066747665405273\n",
      "iter scaler 0.2364492416381836\n",
      "iter start 8.000266075134277\n",
      "iter get data 0.0006673336029052734\n",
      "iter img2gpu 0.009376764297485352\n",
      "iter output 0.022435665130615234\n",
      "roll 0.00016927719116210938\n",
      "make label 0.10432100296020508\n",
      "ssim loss 0.0006697177886962891\n",
      "pop time 8.654594421386719e-05\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 8.034706115722656e-05\n",
      "target loss time 0.0004703998565673828\n",
      "iter loss 0.10624909400939941\n",
      "iter scaler 0.2400805950164795\n",
      "iter start 0.0003681182861328125\n",
      "iter get data 0.0004444122314453125\n",
      "iter img2gpu 0.00927114486694336\n",
      "iter output 0.02234959602355957\n",
      "roll 0.00014519691467285156\n",
      "make label 0.09893035888671875\n",
      "ssim loss 0.0005481243133544922\n",
      "pop time 4.291534423828125e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.058547973632812e-05\n",
      "target loss time 0.0004544258117675781\n",
      "iter loss 0.10060954093933105\n",
      "iter scaler 0.23371195793151855\n",
      "iter start 0.00028967857360839844\n",
      "iter get data 0.0004553794860839844\n",
      "iter img2gpu 0.0024132728576660156\n",
      "iter output 0.00760340690612793\n",
      "roll 0.00014638900756835938\n",
      "make label 0.01495981216430664\n",
      "ssim loss 0.00061798095703125\n",
      "pop time 5.9604644775390625e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.296966552734375e-05\n",
      "target loss time 0.0004589557647705078\n",
      "iter loss 0.016704082489013672\n",
      "iter scaler 0.06333017349243164\n",
      "mean loss 0.00032782554626464844\n",
      "train time 31.504790544509888\n",
      "roll 3.0994415283203125e-05\n",
      "make label 0.09605169296264648\n",
      "ssim loss 0.0011591911315917969\n",
      "pop time 0.00022482872009277344\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 9.846687316894531e-05\n",
      "target loss time 0.0006759166717529297\n",
      "roll 1.4066696166992188e-05\n",
      "make label 0.09484601020812988\n",
      "ssim loss 0.0008795261383056641\n",
      "pop time 6.341934204101562e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.606910705566406e-05\n",
      "target loss time 0.0005941390991210938\n",
      "roll 1.5735626220703125e-05\n",
      "make label 0.09154415130615234\n",
      "ssim loss 0.0007083415985107422\n",
      "pop time 0.0002219676971435547\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 7.271766662597656e-05\n",
      "target loss time 0.0005891323089599609\n",
      "roll 1.3589859008789062e-05\n",
      "make label 0.010034322738647461\n",
      "ssim loss 0.0006682872772216797\n",
      "pop time 5.9604644775390625e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 7.081031799316406e-05\n",
      "target loss time 0.0005910396575927734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|                                                                                                                                     | 1/20 [00:47<14:57, 47.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.2663 / 0.5547                  None\n",
      "contrastive_loss_negative  0.7316 / 0.4453                  None\n",
      "sex                        1.2661 / 0.7269     36.6071 / 43.7500\n",
      "Epoch 1. Current learning rate 0.001. Took 47.25 sec\n",
      "train start\n",
      "iter start 16.620914697647095\n",
      "iter get data 0.02131795883178711\n",
      "iter img2gpu 0.009673595428466797\n",
      "iter output 0.024402379989624023\n",
      "roll 4.863739013671875e-05\n",
      "make label 0.1035158634185791\n",
      "ssim loss 0.0014176368713378906\n",
      "pop time 0.00030159950256347656\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.00013637542724609375\n",
      "target loss time 0.0009634494781494141\n",
      "iter loss 0.10711932182312012\n",
      "iter scaler 0.24752593040466309\n",
      "iter start 0.0006124973297119141\n",
      "iter get data 0.0007953643798828125\n",
      "iter img2gpu 0.012161493301391602\n",
      "iter output 0.024129152297973633\n",
      "roll 4.076957702636719e-05\n",
      "make label 0.09801721572875977\n",
      "ssim loss 0.0014886856079101562\n",
      "pop time 0.0003886222839355469\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.00012874603271484375\n",
      "target loss time 0.0010082721710205078\n",
      "iter loss 0.10207128524780273\n",
      "iter scaler 0.2378220558166504\n",
      "iter start 0.0005211830139160156\n",
      "iter get data 0.0006539821624755859\n",
      "iter img2gpu 0.009582281112670898\n",
      "iter output 0.023919343948364258\n",
      "roll 3.933906555175781e-05\n",
      "make label 0.09827733039855957\n",
      "ssim loss 0.0011932849884033203\n",
      "pop time 0.00030303001403808594\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 0.00011754035949707031\n",
      "target loss time 0.0008525848388671875\n",
      "iter loss 0.10165047645568848\n",
      "iter scaler 0.24420475959777832\n",
      "iter start 0.0005962848663330078\n",
      "iter get data 0.0008027553558349609\n",
      "iter img2gpu 0.009613990783691406\n",
      "iter output 0.024143695831298828\n",
      "roll 4.172325134277344e-05\n",
      "make label 0.0981597900390625\n",
      "ssim loss 0.0014374256134033203\n",
      "pop time 0.0003561973571777344\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.00014352798461914062\n",
      "target loss time 0.000986337661743164\n",
      "iter loss 0.10213947296142578\n",
      "iter scaler 0.23882770538330078\n",
      "iter start 9.523653745651245\n",
      "iter get data 0.001191854476928711\n",
      "iter img2gpu 0.00937032699584961\n",
      "iter output 0.023277997970581055\n",
      "roll 0.0002319812774658203\n",
      "make label 0.10413146018981934\n",
      "ssim loss 0.0010077953338623047\n",
      "pop time 4.9591064453125e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.487701416015625e-05\n",
      "target loss time 0.0005290508270263672\n",
      "iter loss 0.1066279411315918\n",
      "iter scaler 0.24274349212646484\n",
      "iter start 0.10770058631896973\n",
      "iter get data 0.0014238357543945312\n",
      "iter img2gpu 0.009436368942260742\n",
      "iter output 0.022781848907470703\n",
      "roll 2.0265579223632812e-05\n",
      "make label 0.0993201732635498\n",
      "ssim loss 0.0008764266967773438\n",
      "pop time 5.888938903808594e-05\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 8.177757263183594e-05\n",
      "target loss time 0.0007476806640625\n",
      "iter loss 0.10190629959106445\n",
      "iter scaler 0.24102210998535156\n",
      "iter start 0.0005350112915039062\n",
      "iter get data 0.0005438327789306641\n",
      "iter img2gpu 0.0025916099548339844\n",
      "iter output 0.0037364959716796875\n",
      "roll 0.000186920166015625\n",
      "make label 0.01904773712158203\n",
      "ssim loss 0.0007379055023193359\n",
      "pop time 0.0002079010009765625\n",
      "contrastive loss time -7.152557373046875e-07\n",
      "time calc acc 7.915496826171875e-05\n",
      "target loss time 0.0006194114685058594\n",
      "iter loss 0.021752595901489258\n",
      "iter scaler 0.06569480895996094\n",
      "mean loss 0.00019073486328125\n",
      "train time 28.78366780281067\n",
      "roll 2.288818359375e-05\n",
      "make label 0.0936899185180664\n",
      "ssim loss 0.0008749961853027344\n",
      "pop time 0.00024318695068359375\n",
      "contrastive loss time -7.152557373046875e-07\n",
      "time calc acc 9.703636169433594e-05\n",
      "target loss time 0.0007009506225585938\n",
      "roll 1.7881393432617188e-05\n",
      "make label 0.09174180030822754\n",
      "ssim loss 0.0008175373077392578\n",
      "pop time 3.933906555175781e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 7.271766662597656e-05\n",
      "target loss time 0.0005686283111572266\n",
      "roll 1.2636184692382812e-05\n",
      "make label 0.09188675880432129\n",
      "ssim loss 0.0006844997406005859\n",
      "pop time 5.841255187988281e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 7.104873657226562e-05\n",
      "target loss time 0.0005803108215332031\n",
      "roll 1.2636184692382812e-05\n",
      "make label 0.008723735809326172\n",
      "ssim loss 0.0005266666412353516\n",
      "pop time 3.838539123535156e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 6.461143493652344e-05\n",
      "target loss time 0.0005450248718261719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|                                                                                                                              | 2/20 [01:31<13:40, 45.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.2678 / 0.6116                  None\n",
      "contrastive_loss_negative  0.7301 / 0.3884                  None\n",
      "sex                        0.6369 / 0.7296     61.6071 / 43.7500\n",
      "Epoch 2. Current learning rate 0.001. Took 44.43 sec\n",
      "train start\n",
      "iter start 17.91878652572632\n",
      "iter get data 0.03222203254699707\n",
      "iter img2gpu 0.00966954231262207\n",
      "iter output 0.03713107109069824\n",
      "roll 5.817413330078125e-05\n",
      "make label 0.10334205627441406\n",
      "ssim loss 0.001489877700805664\n",
      "pop time 0.0003304481506347656\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.00011754035949707031\n",
      "target loss time 0.00087738037109375\n",
      "iter loss 0.10704159736633301\n",
      "iter scaler 0.248978853225708\n",
      "iter start 0.0005631446838378906\n",
      "iter get data 0.0006811618804931641\n",
      "iter img2gpu 0.009537220001220703\n",
      "iter output 0.02334451675415039\n",
      "roll 3.8623809814453125e-05\n",
      "make label 0.09868907928466797\n",
      "ssim loss 0.0013599395751953125\n",
      "pop time 0.00036644935607910156\n",
      "contrastive loss time -7.152557373046875e-07\n",
      "time calc acc 0.00013971328735351562\n",
      "target loss time 0.0011019706726074219\n",
      "iter loss 0.10262346267700195\n",
      "iter scaler 0.23774194717407227\n",
      "iter start 0.0005040168762207031\n",
      "iter get data 0.0006456375122070312\n",
      "iter img2gpu 0.009512901306152344\n",
      "iter output 0.023772478103637695\n",
      "roll 3.814697265625e-05\n",
      "make label 0.0977928638458252\n",
      "ssim loss 0.0012352466583251953\n",
      "pop time 0.0002906322479248047\n",
      "contrastive loss time -7.152557373046875e-07\n",
      "time calc acc 0.00013017654418945312\n",
      "target loss time 0.0008423328399658203\n",
      "iter loss 0.10113358497619629\n",
      "iter scaler 0.24123525619506836\n",
      "iter start 0.0005109310150146484\n",
      "iter get data 0.0006701946258544922\n",
      "iter img2gpu 0.009521961212158203\n",
      "iter output 0.023384809494018555\n",
      "roll 4.124641418457031e-05\n",
      "make label 0.09857702255249023\n",
      "ssim loss 0.0016663074493408203\n",
      "pop time 0.0003657341003417969\n",
      "contrastive loss time -7.152557373046875e-07\n",
      "time calc acc 0.00018715858459472656\n",
      "target loss time 0.0010962486267089844\n",
      "iter loss 0.10287618637084961\n",
      "iter scaler 0.2396104335784912\n",
      "iter start 9.599050998687744\n",
      "iter get data 0.001584768295288086\n",
      "iter img2gpu 0.009416341781616211\n",
      "iter output 0.023501873016357422\n",
      "roll 0.00017023086547851562\n",
      "make label 0.10431241989135742\n",
      "ssim loss 0.0008783340454101562\n",
      "pop time 0.0001437664031982422\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 8.654594421386719e-05\n",
      "target loss time 0.0005090236663818359\n",
      "iter loss 0.10654234886169434\n",
      "iter scaler 0.23870396614074707\n",
      "iter start 0.33209753036499023\n",
      "iter get data 0.0020494461059570312\n",
      "iter img2gpu 0.009503841400146484\n",
      "iter output 0.02400994300842285\n",
      "roll 0.0002193450927734375\n",
      "make label 0.09860658645629883\n",
      "ssim loss 0.0006375312805175781\n",
      "pop time 6.151199340820312e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 7.867813110351562e-05\n",
      "target loss time 0.0004532337188720703\n",
      "iter loss 0.10063815116882324\n",
      "iter scaler 0.2404472827911377\n",
      "iter start 0.0003750324249267578\n",
      "iter get data 0.0004603862762451172\n",
      "iter img2gpu 0.0024061203002929688\n",
      "iter output 0.003661632537841797\n",
      "roll 0.00014638900756835938\n",
      "make label 0.018902301788330078\n",
      "ssim loss 0.0006155967712402344\n",
      "pop time 5.7220458984375e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.273124694824219e-05\n",
      "target loss time 0.0004642009735107422\n",
      "iter loss 0.020690441131591797\n",
      "iter scaler 0.06253385543823242\n",
      "mean loss 0.0001926422119140625\n",
      "train time 30.393380165100098\n",
      "roll 2.2172927856445312e-05\n",
      "make label 0.09585022926330566\n",
      "ssim loss 0.0009453296661376953\n",
      "pop time 0.0002300739288330078\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 9.799003601074219e-05\n",
      "target loss time 0.0008480548858642578\n",
      "roll 1.430511474609375e-05\n",
      "make label 0.09642267227172852\n",
      "ssim loss 0.0008604526519775391\n",
      "pop time 6.008148193359375e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.511543273925781e-05\n",
      "target loss time 0.0007870197296142578\n",
      "roll 1.52587890625e-05\n",
      "make label 0.09207034111022949\n",
      "ssim loss 0.0009913444519042969\n",
      "pop time 9.989738464355469e-05\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 8.511543273925781e-05\n",
      "target loss time 0.0009102821350097656\n",
      "roll 1.4543533325195312e-05\n",
      "make label 0.008446931838989258\n",
      "ssim loss 0.0006527900695800781\n",
      "pop time 0.0002129077911376953\n",
      "contrastive loss time -2.384185791015625e-07\n",
      "time calc acc 7.367134094238281e-05\n",
      "target loss time 0.0008289813995361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|                                                                                                                       | 3/20 [02:17<12:58, 45.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Loss (train/val) R2 or ACC (train/val)\n",
      "contrastive_loss_positive  0.2549 / 0.6474                  None\n",
      "contrastive_loss_negative  0.7416 / 0.3526                  None\n",
      "sex                        0.6051 / 0.7286     69.6429 / 43.7500\n",
      "Epoch 3. Current learning rate 0.001. Took 46.09 sec\n",
      "train start\n",
      "iter start 16.66936421394348\n",
      "iter get data 0.002959012985229492\n",
      "iter img2gpu 0.009458303451538086\n",
      "iter output 0.03520488739013672\n",
      "roll 7.510185241699219e-05\n",
      "make label 0.07361078262329102\n",
      "ssim loss 0.0013861656188964844\n",
      "pop time 0.0003044605255126953\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.0001404285430908203\n",
      "target loss time 0.0009558200836181641\n",
      "iter loss 0.0962514877319336\n",
      "iter scaler 0.24962377548217773\n",
      "iter start 0.0005967617034912109\n",
      "iter get data 0.0009098052978515625\n",
      "iter img2gpu 0.00963592529296875\n",
      "iter output 0.024207115173339844\n",
      "roll 4.57763671875e-05\n",
      "make label 0.09789752960205078\n",
      "ssim loss 0.0014023780822753906\n",
      "pop time 0.0003845691680908203\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.00013184547424316406\n",
      "target loss time 0.0010213851928710938\n",
      "iter loss 0.10202169418334961\n",
      "iter scaler 0.23839640617370605\n",
      "iter start 0.0005426406860351562\n",
      "iter get data 0.0006754398345947266\n",
      "iter img2gpu 0.009579658508300781\n",
      "iter output 0.023885011672973633\n",
      "roll 4.1484832763671875e-05\n",
      "make label 0.09804534912109375\n",
      "ssim loss 0.0016713142395019531\n",
      "pop time 0.00047850608825683594\n",
      "contrastive loss time -4.76837158203125e-07\n",
      "time calc acc 0.0001583099365234375\n",
      "target loss time 0.001146554946899414\n",
      "iter loss 0.1026298999786377\n",
      "iter scaler 0.2402174472808838\n",
      "iter start 0.0008108615875244141\n",
      "iter get data 0.0009605884552001953\n",
      "iter img2gpu 0.009680509567260742\n",
      "iter output 0.024190902709960938\n",
      "roll 4.553794860839844e-05\n",
      "make label 0.09809422492980957\n",
      "ssim loss 0.0016562938690185547\n",
      "pop time 0.0004942417144775391\n",
      "contrastive loss time -7.152557373046875e-07\n",
      "time calc acc 0.0001678466796875\n",
      "target loss time 0.0011587142944335938\n",
      "iter loss 0.1026010513305664\n",
      "iter scaler 0.23800921440124512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                                                                       | 3/20 [02:46<15:41, 55.39s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ========= Setting ========= ##\n",
    "# args = argument_setting()\n",
    "\n",
    "# seed number\n",
    "args.seed = 1234\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.save_dir = os.getcwd() + '/result'\n",
    "partition, subject_data = make_dataset(args)\n",
    "\n",
    "## ========= Run Experiment and saving result ========= ##    \n",
    "time_hash = datetime.datetime.now().time()\n",
    "hash_key = hashlib.sha1(str(time_hash).encode()).hexdigest()[:6]\n",
    "args.exp_name = args.exp_name + f'_{hash_key}'\n",
    "\n",
    "# Run Experiment\n",
    "print(f\"*** Experiment {args.exp_name} Start ***\")\n",
    "setting, result = experiment(partition, subject_data, deepcopy(args))\n",
    "print(\"===== Experiment Setting Report =====\")\n",
    "print(args)\n",
    "\n",
    "# Save result\n",
    "if args.debug == '':\n",
    "    save_exp_result(setting, result)\n",
    "print(\"*** Experiment Done ***\\n\")\n",
    "## ====================================== ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "652ed6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_transform(\n",
    "    transform, parameters, unpack_parameters = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform transformation `transform` with the provided parameters `parameters`.\n",
    "\n",
    "    If `parameters` is a tuple and `unpack_items` is True, each parameter of `parameters` is unpacked\n",
    "    as arguments to `transform`.\n",
    "    Otherwise `parameters` is considered as single argument to `transform`.\n",
    "\n",
    "    Args:\n",
    "        transform: a callable to be used to transform `data`.\n",
    "        parameters: parameters for the `transform`.\n",
    "        unpack_parameters: whether to unpack parameters for `transform`. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        ReturnType: The return type of `transform`.\n",
    "    \"\"\"\n",
    "    print(type(transform), transform)\n",
    "    if isinstance(parameters, tuple) and unpack_parameters:\n",
    "        return transform(*parameters)\n",
    "\n",
    "    return transform(parameters)\n",
    "\n",
    "def apply_transform(\n",
    "    transform,\n",
    "    data,\n",
    "    map_items: bool = True,\n",
    "    unpack_items: bool = False,\n",
    "    log_stats: bool = False):\n",
    "    print(\"hi\")\n",
    "    if isinstance(data, (list, tuple)) and map_items:\n",
    "        print(\"isins\")\n",
    "        return [_apply_transform(transform, item, unpack_items) for item in data]\n",
    "    return _apply_transform(transform, data, unpack_items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1a5094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "<class 'list'> [<function <lambda> at 0x7f496987f790>, <function <lambda> at 0x7f4969915670>, <function <lambda> at 0x7f4969915820>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m transform\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m7\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mapply_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parameters, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mparameters)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "from monai.transforms import Compose\n",
    "from itertools import repeat\n",
    "\n",
    "transform= [lambda x: x**2, lambda x: x+3, lambda x: x*3]\n",
    "img = [3,7]\n",
    "\n",
    "img = list(map(apply_transform, [transform]*2, img, repeat(False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e617a3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itertools.repeat"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(repeat(False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

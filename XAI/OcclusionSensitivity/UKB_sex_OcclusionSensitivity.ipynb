{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load library\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import densenet3d\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt # graph\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from tqdm.auto import tqdm # process bar\n",
    "import random\n",
    "\n",
    "import monai\n",
    "from monai.data import CSVSaver, ImageDataset, DistributedWeightedRandomSampler\n",
    "from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, Flip, ToTensor\n",
    "from monai.utils import set_determinism\n",
    "from monai.apps import CrossValidation\n",
    "import imageio\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.resize = (80,80,80)\n",
    "args.target = 'sex'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orig_image_dir = '/share/master_ssd/3DCNN/data/2.UKB/1.sMRI_fs_cropped'\n",
    "os.chdir(orig_image_dir)\n",
    "orig_image_files = glob.glob('*.nii.gz')\n",
    "orig_image_files = sorted(orig_image_files)\n",
    "\n",
    "female_cam_upper_75_dir = '/share/scratch/connectome/dhkdgmlghks/UKB_interpretation/sex/OcclusionSensitivity/female_upper_0.75'\n",
    "female_cam_upper_75_files = glob.glob(os.path.join(female_cam_upper_75_dir,'*.npy'))\n",
    "female_cam_upper_75_files = sorted(female_cam_upper_75_files)\n",
    "\n",
    "male_cam_upper_75_dir = '/share/scratch/connectome/dhkdgmlghks/UKB_interpretation/sex/OcclusionSensitivity/male_upper_0.75'\n",
    "male_cam_upper_75_files = glob.glob(os.path.join(male_cam_upper_75_dir,'*.npy'))\n",
    "male_cam_upper_75_files = sorted(male_cam_upper_75_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/wangheehwan/Desktop/UKB_sex_GradCAM.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangheehwan/Desktop/UKB_sex_GradCAM.ipynb#ch0000008?line=0'>1</a>\u001b[0m target \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msex\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangheehwan/Desktop/UKB_sex_GradCAM.ipynb#ch0000008?line=1'>2</a>\u001b[0m col_list \u001b[39m=\u001b[39m [target] \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39meid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wangheehwan/Desktop/UKB_sex_GradCAM.ipynb#ch0000008?line=3'>4</a>\u001b[0m subject_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m/share/master_ssd/3DCNN/data/2.UKB/2.demo_qc/UKB_phenotype.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangheehwan/Desktop/UKB_sex_GradCAM.ipynb#ch0000008?line=4'>5</a>\u001b[0m subject_data \u001b[39m=\u001b[39m subject_data\u001b[39m.\u001b[39mloc[:,col_list]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangheehwan/Desktop/UKB_sex_GradCAM.ipynb#ch0000008?line=5'>6</a>\u001b[0m subject_data \u001b[39m=\u001b[39m subject_data\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meid\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "col_list = [args.target] + ['eid']\n",
    "\n",
    "subject_data = pd.read_csv('/share/master_ssd/3DCNN/data/2.UKB/2.demo_qc/UKB_phenotype.csv')\n",
    "subject_data = subject_data.loc[:,col_list]\n",
    "subject_data = subject_data.sort_values(by='eid')\n",
    "subject_data = subject_data.dropna(axis = 0) \n",
    "subject_data = subject_data.reset_index(drop=True) # removing subject have NA values in sex\n",
    "\n",
    "\n",
    "imageFiles_labels = []\n",
    "    \n",
    "    \n",
    "subj= []\n",
    "if type(subject_data['eid'][0]) == np.str_ or type(subject_data['eid'][0]) == str:\n",
    "    for i in range(len(orig_image_files)):\n",
    "        subj.append(str(orig_image_files[i][:-12]))\n",
    "elif type(subject_data['eid'][0]) == np.int_ or type(subject_data['eid'][0]) == int:\n",
    "    for i in range(len(orig_image_files)):\n",
    "        subj.append(int(orig_image_files[i][:-12]))\n",
    "    \n",
    "image_list = pd.DataFrame({'eid':subj, 'image_files': orig_image_files})\n",
    "subject_data = pd.merge(subject_data, image_list, how='inner', on='eid')\n",
    "\n",
    "col_list = col_list + ['image_files']\n",
    "    \n",
    "for i in tqdm(range(len(subject_data))):\n",
    "    imageFile_label = {}\n",
    "    for j, col in enumerate(col_list):\n",
    "        imageFile_label[col] = subject_data[col][i]\n",
    "    imageFiles_labels.append(imageFile_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(imageFiles_labels,args):\n",
    "    #random.shuffle(imageFiles_labels)\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "     \n",
    "\n",
    "    for imageFile_label in imageFiles_labels:\n",
    "        image = imageFile_label['image_files']\n",
    "        label = imageFile_label[args.target]\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    resize = tuple(args.resize)\n",
    "    train_transform = Compose([ScaleIntensity(),\n",
    "                               AddChannel(),\n",
    "                               Resize(resize),\n",
    "                              ToTensor()])\n",
    "\n",
    "    val_transform = Compose([ScaleIntensity(),\n",
    "                               AddChannel(),\n",
    "                               Resize(resize),\n",
    "                              ToTensor()])\n",
    "\n",
    "    test_transform = Compose([ScaleIntensity(),\n",
    "                               AddChannel(),\n",
    "                               Resize(resize),\n",
    "                              ToTensor()])\n",
    "\n",
    "    # number of total / train,val, test\n",
    "    num_total = len(images)\n",
    "    num_train = int(num_total*(1 - args.val_size - args.test_size))\n",
    "    num_val = int(num_total*args.val_size)\n",
    "    num_test = int(num_total*args.test_size)\n",
    "\n",
    "    # image and label information of train\n",
    "    images_train = images[:num_train]\n",
    "    labels_train = labels[:num_train]\n",
    "\n",
    "    # image and label information of valid\n",
    "    images_val = images[num_train:num_train+num_val]\n",
    "    labels_val = labels[num_train:num_train+num_val]\n",
    "\n",
    "    # image and label information of test\n",
    "    images_test = images[num_train+num_val:]\n",
    "    labels_test = labels[num_train+num_val:]\n",
    "\n",
    "    train_set = ImageDataset(image_files=images_train,labels=labels_train,transform=train_transform)\n",
    "    val_set = ImageDataset(image_files=images_val,labels=labels_val,transform=val_transform)\n",
    "    test_set = ImageDataset(image_files=images_test,labels=labels_test,transform=test_transform)\n",
    "\n",
    "    partition = {}\n",
    "    partition['train'] = train_set\n",
    "    partition['val'] = val_set\n",
    "    partition['test'] = test_set\n",
    "\n",
    "    \n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = partition_dataset(imageFiles_labels,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(orig_image_dir)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(partition['test'],\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=2)\n",
    "\n",
    "count = 0\n",
    "for i, data in enumerate(testloader,0):\n",
    "    image, label = data \n",
    "    count += 1 \n",
    "    \n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Mean heat map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### female upper 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heatmap_female = np.zeros((img_size, img_size, img_size))\n",
    "\n",
    "for img_dir in tqdm(female_cam_upper_75_files): \n",
    "    img = np.load(img_dir)\n",
    "    mean_heatmap_female += img \n",
    "\n",
    "\n",
    "mean_heatmap_female = mean_heatmap_female / len(female_cam_upper_75_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid-sagittal image \n",
    "slice_orig = image.cpu().squeeze().numpy()[40,:,:]\n",
    "slice_heat = mean_heatmap_female[40,:,:]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.rot90(slice_orig),interpolation='nearest',cmap=plt.cm.gray)\n",
    "plt.imshow(np.rot90(slice_heat),interpolation='bilinear',cmap='jet',alpha=0.5)\n",
    "\n",
    "# mid-scoronal image\n",
    "slice_orig = image.cpu().squeeze().numpy()[:,40,:]\n",
    "slice_heat = mean_heatmap_female[:,40,:]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.rot90(slice_orig),interpolation='nearest',cmap=plt.cm.gray)\n",
    "plt.imshow(np.rot90(slice_heat),interpolation='bilinear',cmap='jet',alpha=0.5)\n",
    "\n",
    "# mid-horizontal image\n",
    "slice_orig = image.cpu().squeeze().numpy()[:,:,40]\n",
    "slice_heat = mean_heatmap_female[:,:,40]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.rot90(slice_orig),interpolation='nearest',cmap=plt.cm.gray)\n",
    "plt.imshow(np.rot90(slice_heat),interpolation='bilinear',cmap='jet',alpha=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male upper 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heatmap_male = np.zeros((img_size, img_size, img_size))\n",
    "\n",
    "for img_dir in tqdm(male_cam_upper_75_files): \n",
    "    img = np.load(img_dir)\n",
    "    mean_heatmap_male += img \n",
    "\n",
    "\n",
    "mean_heatmap_male = mean_heatmap_male / len(female_cam_upper_75_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid-sagittal image \n",
    "slice_orig = image.cpu().squeeze().numpy()[40,:,:]\n",
    "slice_heat = mean_heatmap_male[40,:,:]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.rot90(slice_orig),interpolation='nearest',cmap=plt.cm.gray)\n",
    "plt.imshow(np.rot90(slice_heat),interpolation='bilinear',cmap='jet',alpha=0.5)\n",
    "\n",
    "# mid-scoronal image\n",
    "slice_orig = image.cpu().squeeze().numpy()[:,40,:]\n",
    "slice_heat = mean_heatmap_male[:,40,:]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.rot90(slice_orig),interpolation='nearest',cmap=plt.cm.gray)\n",
    "plt.imshow(np.rot90(slice_heat),interpolation='bilinear',cmap='jet',alpha=0.5)\n",
    "\n",
    "# mid-horizontal image\n",
    "slice_orig = image.cpu().squeeze().numpy()[:,:,40]\n",
    "slice_heat = mean_heatmap_male[:,:,40]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.rot90(slice_orig),interpolation='nearest',cmap=plt.cm.gray)\n",
    "plt.imshow(np.rot90(slice_heat),interpolation='bilinear',cmap='jet',alpha=0.5)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
